[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I’m Aaron Montgomery, an Associate Professor at Baldwin Wallace University. I received a PhD in Mathematics (specializing in Probability Theory) in 2013 at the University of Oregon under the direction of David Levin. Since then, I have been a member of the BW Mathematics and Statistics faculty, and more recently I have become the coordinator of the new Data Science and Data Analytics programs.\nI am always open to collaborating on interesting statistical or data-oriented projects! You can reach me at amontgom (atsymbol) bw.edu. You can find the GitHub source for this blog here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "This blog is an assorted collection of notes on probability, statistics, data, and coding.\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nComing Soon….\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2023\n\n\nAaron Montgomery\n\n\n\n\n\n\n  \n\n\n\n\nPenney’s Game, pt. 2\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nmonte-carlo\n\n\npenneys-game\n\n\ncounterintuitive\n\n\n\n\nThe waiting (time) is the hardest part\n\n\n\n\n\n\nAug 10, 2022\n\n\nAaron Montgomery\n\n\n\n\n\n\n  \n\n\n\n\nPenney’s Game, pt. 1\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nmonte-carlo\n\n\npenneys-game\n\n\ncounterintuitive\n\n\n\n\nA naturally-occurring rock-paper-scissors\n\n\n\n\n\n\nAug 5, 2022\n\n\nAaron Montgomery\n\n\n\n\n\n\n  \n\n\n\n\nHey, Who Invited You?\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nmonte-carlo\n\n\neulers-number\n\n\ncounterintuitive\n\n\n\n\nA coding problem brings an uninvited famous mathematical guest\n\n\n\n\n\n\nJul 27, 2022\n\n\nAaron Montgomery\n\n\n\n\n\n\n  \n\n\n\n\nLet’s Make a Deal!\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nmonte-carlo\n\n\nmonty-hall\n\n\ncounterintuitive\n\n\n\n\nMonty Hall meets Monte Carlo\n\n\n\n\n\n\nJul 14, 2022\n\n\nAaron Montgomery\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-07-10-monty-hall/index.html",
    "href": "posts/2022-07-10-monty-hall/index.html",
    "title": "Let’s Make a Deal!",
    "section": "",
    "text": "Photo by Sebastian Herrmann on Unsplash"
  },
  {
    "objectID": "posts/2022-07-10-monty-hall/index.html#the-incorrect-answer",
    "href": "posts/2022-07-10-monty-hall/index.html#the-incorrect-answer",
    "title": "Let’s Make a Deal!",
    "section": "The Incorrect Answer",
    "text": "The Incorrect Answer\nI was first exposed to this problem in college when a friend posed the question to me as a brain teaser to at a cafeteria table. I gave the only sensible answer, which was to say that since you had two doors, getting the prize was a 50/50 proposition no matter what you chose. I was, of course, dead wrong.\nFor years, I was slightly embarrassed about this. Not too embarrassed, mind you – after all, I’m wrong all the time. But this one stung because my friend was on my home turf; at this point in my life, I was pretty sure I was going to try to pursue a PhD in Mathematics and I suspected I might gravitate to the field of Probability Theory. Just like that, I whiffed on a probability question in a semi-public forum.\nMy shame was lessened over the years when I learned that if nothing else, I wasn’t alone. When Marilyn Vos Savant gave a correct solution to the problem in a 1990 issue of Parade, she received a truckload of letters, many from professional mathematicians, telling her how wrong she was. (She wasn’t wrong, which certainly didn’t help the outrageous rudeness of some of those letters.) In his book Which Door Has the Cadillac: Adventures of a Real Life Mathematician, Andrew Vazsonyi recalls giving the same incorrect “obvious” answer to the problem on his first encounter; perhaps more shockingly, he details an account of discussing the problem with Paul Erdös, who also got the problem wrong and became increasingly irate about it until he was eventually shown a simulation proving what the right answer should be."
  },
  {
    "objectID": "posts/2022-07-10-monty-hall/index.html#the-correct-answer",
    "href": "posts/2022-07-10-monty-hall/index.html#the-correct-answer",
    "title": "Let’s Make a Deal!",
    "section": "The Correct Answer",
    "text": "The Correct Answer\nThat right answer is that switching is better. Indeed, staying with your original choice will grant you a \\(1/3\\) chance of winning, and switching will grant a \\(2/3\\) chance of winning. The key detail, of course, is Monty’s knowledge of the prize location and his choice of exactly how to reveal what he knows. There are many ways to see why this is true; the Wikipedia entry for the Monty Hall problem gives many different explanations of many different flavors (and even criticisms of those same explanations). These explanations are great, but to those who aren’t accustomed to long mathematical arguments, they might be less than convincing.\nThe first explanation of the solution in the Wikipedia article states:\n\nWhen the player first makes their choice, there is a \\(2/3\\) chance that the car is behind one of the doors not chosen. This probability does not change after the host reveals a goat behind one of the unchosen doors.\n\nThis explanation never quite sat right with me. Sasha Volokh expressed my vague concern quite well:\n\nFirst, it’s clear that any explanation that says something like “the probability of door 1 was 1/3, and nothing can change that…” is automatically fishy: probabilities are expressions of our ignorance about the world, and new information can change the extent of our ignorance.\n\nThis is a case where simulations can do us some good."
  },
  {
    "objectID": "posts/2022-07-10-monty-hall/index.html#simulations",
    "href": "posts/2022-07-10-monty-hall/index.html#simulations",
    "title": "Let’s Make a Deal!",
    "section": "Simulations",
    "text": "Simulations\nWe’ll write Monte Carlo simulations in R to see that by sticking with our original answer, the probability of winning is indeed \\(1/3\\). We will write a function that simulates one full round of the game; then, we’ll replicate() the function many times to determine the probability of winning. Our strategy will be to simulate a round of the full game many times and keep track of how often the game results in a win.\nFor our first attempt, we’ll recreate a very general version of the game:\n\nthe prize can be found behind any of 3 doors\nthe contestant will pick any of 3 doors\nMonty will reveal one door and offer a chance to switch\nthe contestant will choose to switch or stay depending on the parameter stay\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\nmonty_hall <- function(stay){\n    \n  doors <- c(\"A\", \"B\", \"C\")              # using ABC due to a quirk in sample()\n\n  prize_door <- sample(doors, 1)\n  contestant_choice <- sample(doors, 1)\n    \n  reveal_door <-                                # Monty can't reveal door with the prize,\n    union(prize_door, contestant_choice) %>%    # nor the selected door, so...\n    setdiff(doors, .) %>%                       # we remove those choices \n    sample(1)  \n\n  switch_offer <- setdiff(doors, c(contestant_choice, reveal_door))\n    \n  ifelse(prize_door == ifelse(stay, contestant_choice, switch_offer),\n         \"win\", \"lose\")\n    # the function returns the strings \"win\" and \"lose\"\n    # when stay is TRUE, check if the prize door matches the contestant's choice\n    # when stay is FALSE, check if the prize door matches the door offered in a switch\n}\n\nNext, we’ll generate 10K trials of the game under each of the two options (switching and staying). We’ll reshape the outcomes just a bit and then plot them.\n\nmonty_stay_trials <- replicate(10000, monty_hall(stay = TRUE))\nmonty_switch_trials <- replicate(10000, monty_hall(stay = FALSE))\n\ndata.frame(stay = monty_stay_trials, switch = monty_switch_trials) %>%\n  pivot_longer(everything(), names_to = \"choice\", values_to = \"outcome\") %>% \n  ggplot(aes(x = outcome)) +\n    geom_bar(stat = \"count\") +\n    facet_wrap(~choice) +\n    labs(title = \"Monty Hall Monte Carlo simulation results\",\n         subtitle = \"10,000 trials per contestent choice possibility\") +\n    theme(text = element_text(size = 20))\n\n\n\n\nThe simulations show us exactly what we expected; switching is good, staying is bad. We can also easily confirm that the probability of winning by switching is \\(2/3\\):\n\nmean(monty_switch_trials == \"win\")\n\n[1] 0.6657\n\n\nOur answer is close to \\(2/3\\), and the difference between it and \\(2/3\\) is a small statistical fluctuation, as expected.\nSo, good news: we have confirmed that the correct answer is indeed correct. But can we render any deeper insights from this? Let’s focus on the function in the case when stay == TRUE. In that case, the code looks like this:\nmonty_hall <- function(stay){\n    \n  doors <- c(\"A\", \"B\", \"C\")      \n\n  prize_door <- sample(doors, 1)\n  contestant_choice <- sample(doors, 1)\n    \n  reveal_door <- \n    union(prize_door, contestant_choice) %>%    \n    setdiff(doors, .) %>%                       \n    sample(1)  \n\n  switch_offer <- setdiff(doors, c(contestant_choice, reveal_door))\n    \n  ifelse(prize_door == contestant_choice, \"win\", \"lose\")\n}\n\nHere, switch_offer doesn’t actually get used at all in the line that returns the function. This means that the switch_offer <- ... line, and the reveal_door <- ... lines, are unused appendages. If we remove them, we’re left with this:\nmonty_hall <- function(stay){\n    \n  doors <- c(\"A\", \"B\", \"C\")      \n\n  prize_door <- sample(doors, 1)\n  contestant_choice <- sample(doors, 1)\n    \n  ifelse(prize_door == contestant_choice, \"win\", \"lose\")\n}\nNow, we see that our code has reduced to a simulation that draws two objects independently from a collection of three and checks to see if they’re the same. That probability is clearly \\(1/3\\), and this explanation now aligns perfectly with the Wikipedia explanation that nothing has changed the original probability of \\(1/3\\), whether that explanation is “fishy” or not."
  },
  {
    "objectID": "posts/2022-07-10-monty-hall/index.html#takeaway",
    "href": "posts/2022-07-10-monty-hall/index.html#takeaway",
    "title": "Let’s Make a Deal!",
    "section": "Takeaway",
    "text": "Takeaway\nIt’s great to be able to use Monte Carlo simulations to confirm a correct answer, but in this case the act of writing a simulation can do something more profound: it can make the why behind the answer just a bit more convincing. Or, at least, it did that for me. I’ve seen (and believed, and produced) many analytical arguments for why switching has a \\(2/3\\) probability, but I never fully believed the Wikipedia explanation until writing code to simulate the game."
  },
  {
    "objectID": "posts/2022-07-27-ordered-objects/index.html",
    "href": "posts/2022-07-27-ordered-objects/index.html",
    "title": "Hey, Who Invited You?",
    "section": "",
    "text": "Photo by Surendran MP on Unsplash"
  },
  {
    "objectID": "posts/2022-07-27-ordered-objects/index.html#the-incorrect-answer",
    "href": "posts/2022-07-27-ordered-objects/index.html#the-incorrect-answer",
    "title": "Hey, Who Invited You?",
    "section": "The Incorrect Answer",
    "text": "The Incorrect Answer\nMy immediate answer was: No, that must just be a coincidence."
  },
  {
    "objectID": "posts/2022-07-27-ordered-objects/index.html#the-correct-answer",
    "href": "posts/2022-07-27-ordered-objects/index.html#the-correct-answer",
    "title": "Hey, Who Invited You?",
    "section": "The Correct Answer",
    "text": "The Correct Answer\nI was wrong. The true answer was actually \\(e\\). Well, almost.\n(Quick disclaimer: this result is perfectly well-known to humanity; it just wasn’t well-known to me at the time.)\nWhen the student asked me this question, I replicated my answer a few more times to see if I still thought the resemblence to the magic number \\(2.718282...\\) was accidental.\n\nmean(replicate(10000, letters_until_unordered()))\n\n[1] 2.7228\n\n\nHmmm…\n\nmean(replicate(10000, letters_until_unordered()))\n\n[1] 2.726\n\n\nVery suspicious. Let’s try using more replications for finer accuracy:\n\nmean(replicate(1e5, letters_until_unordered()))\n\n[1] 2.71936\n\n\nHMMMM…..\n\nmean(replicate(1e6, letters_until_unordered())) \n\n[1] 2.717368\n\n# runtime starts to get a little long: ~50 seconds on my machine\n\nThat was hard to ignore, and it practically reeked of Euler’s number. This caught me completely off guard; I had written the problem without any idea that \\(e\\) belonged here at all, and yet… here it was. On the one hand, I shouldn’t have been surprised at all. This is just a Thing That Happens with certain famous mathematical constants like \\(e\\) and \\(\\pi\\); they frequently show up where they frankly have no business being.\nI did find myself surprised, though – mostly, by the fact that I had encountered an experiment-driven result, which I am not at all accustomed to. I don’t think I would ever have considered this result were it not for this esoteric homework problem I conjured up (and the help of the keen-eyed student). Monte Carlo simulations gave me something that I imagine must feel a little bit like when a physicist can’t explain a bump on a curve, or when a biologist sees a bacterial growth rate that has no known explanation. The next step for that physicist, or that biologist, and for me, was to ask why.\nIn this way, I was behaving in the most analogous way possible to an experimental scientist. Such a scientist might discover a fascinating new result in a lab; new results yearn for new theories, and it is those theories that are powerful. Theories are what convey existing and lasting knowledge to humanity. The experiments themselves matter only insofar as they suggest or reveal deeper truths; but the experiments themselves are but partial reflections of those truths. This homework problem doesn’t matter; what makes it tick matters.\nThis also helped crystallize to me an important lesson about the pedagogy of mathematical education. One of my most important missions in my own classrooms has been to transfer as much agency to students as possible. I tend to gravitate strongly toward active learning strategies. My teaching philosophy is fairly simple: anything students do is good, and anything I do is inherently less good. Here, too, I’ve long been jealous of lab sciences; they have a built-in pedagogy to invite students to meaningfully engage, right from the very beginning of study. Teaching a course on Monte Carlo simulations has been the first thing I’ve done that has felt just a little bit like a lab.\nAnd yet, the simulations have also underscored for me the importance of understanding “traditional” mathematics; we can discover wildly interesting results through Monte Carlo simulations, but only mathematical reasoning can deliver the all-important why. Simulations are a wonderful way to augment a traditional probability and statistics curriculum, but I don’t think they should replace it."
  },
  {
    "objectID": "posts/2022-07-27-ordered-objects/index.html#takeaway",
    "href": "posts/2022-07-27-ordered-objects/index.html#takeaway",
    "title": "Hey, Who Invited You?",
    "section": "Takeaway",
    "text": "Takeaway\nMonte Carlo simulations have given me a new way to invite students who can write just a bit of code to experiment and to play in a mathematical space. Inviting students into this style of mathematical “laboratory” has been incredibly rewarding, and it has allowed me to engage students much more fully than I’m typically able to do in introductory probability / statistics classes. It has allowed me to transfer some of the ownership of the subject material to students, which seems to me an unqualified good thing.\nHowever, Monte Carlo simulations are inherently limited in scope. They can give us incredibly interesting results, and they can do so in a much more accessible way than can classical mathematics and statistics. But by themselves, they can never give us that elusive why."
  },
  {
    "objectID": "posts/2022-07-27-ordered-objects/index.html#wait-what-about-the-why-part",
    "href": "posts/2022-07-27-ordered-objects/index.html#wait-what-about-the-why-part",
    "title": "Hey, Who Invited You?",
    "section": "Wait, what about the “Why” part?",
    "text": "Wait, what about the “Why” part?\nIt would be pretty unsatisfying if after all that discussion, we didn’t get around to the proof of the aforementioned result. Once again, to be clear, this result is perfectly well-known; I just hadn’t personally encountered it before. Once the student brought this seeming coincidence to my attention, I immediately set out to try to prove it.\nTo remind ourselves, here’s the result:\n\nTheorem\nSuppose you repeatedly draw upper-case letters from a jar without replacement until the first time you get one that is out of order. The expected number of draws executed in this fashion will be very slightly less than \\(e\\).\n\n\nProof\nFor \\(k \\geq 1\\), let \\(A_k\\) denote the event that the first \\(k\\) letters drawn are all in order. (That is: \\(A_1\\) is trivial, \\(A_2\\) holds only if the first two letters are drawn in order, and so forth.) If we let \\(N\\) denote random variable of the number of letters drawn until finding one out of sequence, then we note that \\[ N = 1 + \\mathbb 1_{A_1} + \\mathbb 1_{A_2} + \\dots + \\mathbb 1_{A_{26}}\\] where \\(\\mathbb 1_{B}\\) denotes the indicator function of event \\(B\\). This is both the most important and least obvious step in the proof; the logic of this step is that \\(N\\) and each \\(\\mathbb 1_{A_k}\\) are random variables, meaning they’re functions of some unseen input variable \\(\\omega\\). Here, choosing an \\(\\omega\\) determines a full ordering of the letters, and once this has been done, a certain number of events \\(\\{A_1, \\dots, A_n\\}\\) will be fulfilled, making their corresponding indicator variables evaluate to \\(1\\) on that \\(\\omega\\). Summing these indicators will therefore count the events, as desired.\nThe leading \\(1\\) in the above expression is meant to count the one letter we’ll draw that’s out of order and therefore not counted by the indicator variables; technically, this isn’t very well-defined for the case when we draw all 26 letters in order, but that’s a very low-probability event that we were explicitly invited in the problem statement to ignore, so we’ll just accept the fact that \\(N\\) evaluates to \\(27\\) in that case. (I originally added that detail in the problem statement to alleviate a coding nuisance, and it turned out to alleviate a corresponding mathematical nuisance here.)\nThe decomposition above is useful, because it therefore follows that \\[ \\mathbb E[N] = 1 + \\mathbb E \\left[ \\mathbb 1_{A_1} \\right] + \\mathbb E \\left[ \\mathbb 1_{A_2} \\right] + \\dots + \\mathbb E \\left[ \\mathbb 1_{A_{26}} \\right] = 1 + \\sum_{k=1}^{26} \\mathbb E \\left[ \\mathbb 1_{A_k} \\right].\\] I should stop and explicitly acknowledge that this really, really feels like we’re cheating somehow. The \\(\\{A_k\\}\\) events are definitely not independent; in fact, they’re completely dependent, as \\(A_{k+1} \\subset A_k\\). (If the first twelve letters were perfectly ordered, then the first eleven were also.) However, independence is not required for linearity of expectation.\nFrom here, we recall that \\(\\mathbb E \\left[ \\mathbb 1_B \\right] = \\mathbb P(B)\\) for any event \\(B\\), which means that we need only to compute \\(\\mathbb P(A_k)\\). When drawing \\(k\\) objects, there are \\(k!\\) rearrangements of those objects, of which only \\(1\\) is properly ordered, from which it follows that \\(\\mathbb P(A_k) = \\frac 1 {k!}\\). Hence, \\[\\mathbb E[N] = 1 + \\sum_{k=1}^{26} \\mathbb P \\left( A_k \\right) = 1 + \\sum_{k=1}^{26} \\frac{1}{k!} = \\sum_{k=0}^{26} \\frac{1}{k!}\\] and we notice that this is simply the first few terms of the infinite series \\[\\sum_{k=0}^{\\infty} \\frac{1}{k!} = e.\\]\nSo, the true expected value in our original question isn’t quite \\(e\\) – it’s a number just below it. Specifically, it’s \\(e - \\sum_{k=27}^{\\infty} \\frac{1}{k!}\\). To see how close this is to \\(e\\), let’s use R to sum the first few terms of the remainder series:\n\nsum(1 / factorial(27:1000))\n\n[1] 9.523378e-29\n\n\nPretty close to \\(e\\), indeed. And while the true answer isn’t actually quite \\(e\\), we were never going to be able to distinguish the true answer from \\(e\\) with only a Monte Carlo simulation."
  },
  {
    "objectID": "posts/2022-08-05-penneys-game-1/index.html",
    "href": "posts/2022-08-05-penneys-game-1/index.html",
    "title": "Penney’s Game, pt. 1",
    "section": "",
    "text": "Photo by Dan Dennis on Unsplash \n\n\n\n\nPenney’s Game\nThe setup: you’re playing a game with a friend involving coin flips. Both you and your friend choose a sequence of three coin flips; let’s say that you choose THH, and your friend chooses HHT. A neutral referee flips a single coin repeatedly until one of you encounters the sequence you’re waiting for. So, for instance, if the sequence of coin flips is\n\nTHTTHTHTHH\n\nthen you would win the game, because THH occurred before HHT ever did. The question is: is this a fair game? Or, does one player have an advantage over the other?\n\n\nThe Incorrect Answer\nThis question, introduced by Walter Penney (hence the name), is a great question to trip up some people who know some probability. When I first heard this question, the answer seemed obvious – of course it’s fair! After all, I knew that in a sequence of of three isolated coin flips, both THH and HHT (and any other sequence of coin flips) had a \\(\\frac 1 2 \\cdot \\frac 1 2 \\cdot \\frac 1 2 = \\frac 1 8\\) chance of appearing. That claim is of course true, but in the spirit of a Mathematical Laboratory, let’s take the opportunity to visualize it with some quick Monte Carlo simulations:\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\n\n\n# the first line makes a vector of 10,000 strings like \"HTH\" or \"TTT\"\n\nreplicate(1e5, paste0(sample(c(\"H\", \"T\"), 3, replace = T), collapse = \"\")) %>%\n  tibble(sequence = .) %>%\n  arrange(sequence) %>%\n  ggplot(aes(x = sequence)) +\n    geom_bar(stat = \"count\") +\n    labs(title = \"Monte Carlo simulations of flipping 3 coins\",\n         subtitle = \"10,000 total trials\") +\n    theme(text = element_text(size = 20))\n\n\n\n\nAs expected, we encounter each of the 8 distinct sequences with approximately equal frequency, so we haven’t gone astray yet. The problem arises when we try to extend that logic to the actual game described up above; in this case, we’re waiting for one of two sequences to appear instead of just stopping after 3 flips and logging the outcomes. Here are the simulations for the THH vs. HHT game we originally described:\n\n# This function will take two strings of coin flips and return the one that occurs\n# first in a long sequence of coin flips.\n\npenney <- function(p1, p2){         # p1 and p2 are strings like \"HHT\", \"HTT\"\n  p1 <- unlist(str_split(p1, \"\"))   # split string into vector of length 3\n  p2 <- unlist(str_split(p2, \"\"))\n  flips <- sample(c(\"H\", \"T\"), 3, replace = T) \n    # initialize flips with first 3 coin flips\n    \n    # on each pass of the while loop, push the first two coins forward in the list,\n    # then sample a new coin for the third one; for instance, HTH becomes TH? where\n    # ? is the result of a new coin flip\n    \n  while(!all(p1 == flips) & !all(p2 == flips)){\n    flips[1:2] <- flips[2:3]\n    flips[3] <- sample(c(\"H\", \"T\"), 1)\n  }\n    \n  ifelse(all(p1 == flips), \n         paste0(p1, collapse = \"\"), \n         paste0(p2, collapse = \"\"))  # output winning sequence\n}\n\n\nreplicate(10000, penney(\"HHT\", \"THH\")) %>%\n  tibble(winner = .) %>%\n  arrange(winner) %>%\n  ggplot(aes(x = winner)) +\n    geom_bar(stat = \"count\") +\n    labs(title = \"Monte Carlo simulations of Penney's Game\",\n         subtitle = \"10,000 total trials: HHT vs. THH\") +\n    theme(text = element_text(size = 20))\n\n\n\n\nIt is immediately clear that this is not a fair game.\n\n\nThe Correct Answer\nOnce we see that the game is not fair, we should ask why. For these particular two coin flips, there’s a good argument of why HHT should beat THH just one out of four times: the only way that HHT can win is if the first two coin flips are both heads. Otherwise, there’s some first occurrence of two H’s in the sequence, and that first occurrence is necessarily preceded by a T. The probability that this occurs is \\(\\frac 1 2 \\cdot \\frac 1 2 = \\frac 1 4\\), which is why HHT won about 2500 times out of our 10,000 simulations above.\nAfter we understand this head-to-head matchup, the obvious question is: what about all possible head-to-head matchups? To see, we’ll return to the “laboratory” by simulating every possible matchup of all three sequences and generating a heatmap of the results.\n\n# this helper function takes two coin sequences, simulates Penney's game many\n# times, and returns the proportion of times the *first* sequence wins\npenney_wrapper <- function(p1, p2){\n  mean(replicate(1e4, penney(p1, p2)) == p1)\n}\n\n# make a data frame with all combinations of sequences\nresults <-\n  c(\"HHH\", \"HHT\", \"HTH\", \"HTT\", \"THH\", \"THT\", \"TTH\", \"TTT\") %>%\n  combn(2) %>%\n  t() %>%\n  as.data.frame()\n\nnames(results) <- c(\"p1\", \"p2\")\n\nresults <- \n  results %>%\n  rowwise() %>%\n  mutate(p1_wins = penney_wrapper(p1, p2)) %>%\n  mutate(p1_wins = round(p1_wins, 2))\n\n# since we simulated only half of the matchups, we can just mirror\n# the results to flip p1 and p2 rather than resimulating\nresults <-\n  tibble(\n    p1 = results$p2,\n    p2 = results$p1,\n    p1_wins = 1 - results$p1_wins\n  ) %>%\n    bind_rows(results)\n\nresults %>%\n  ggplot(aes(x = p1, y = p2, fill = p1_wins)) +\n    geom_tile() +\n    geom_text(aes(label = p1_wins), size = 6) +\n    scale_fill_gradient2(low = \"yellow\", mid = \"white\", \n                         high = \"cyan\", midpoint = 0.5) +\n    labs(title = \"Head-to-head matchups: Penney's Game\",\n         subtitle = \"10K trials per matchup combination\",\n         fill = \"p1 win %\") +\n    theme(text = element_text(size = 20))\n\n\n\n\nThis chart reveals some pretty fascinating truths about Penney’s Game. Here are some of the things we learn:\n\n1. On the whole, Penney’s Game is certainly not fair.\nWe can see that some matchups have… * slight advantages, like THH over TTT * moderate advantages, like HHT over TTT * overwhelming advantages, like HTT over TTT\n\n\n2. Yet, fair games do exist within Penney’s Game.\nNo matter which sequence Player 1 picks, Player 2 can pick something that has a \\(50\\%\\) chance of beating it; one way to do this is to take the inverse of Player 1’s sequence, like picking TTH in response to HHT.\n\n\n3. Rock-Paper-Scissors\nOn the other hand, if Player 1 picks their sequence first, then no matter what they have picked, Player 1 can pick something that has an advantage over it. This is almost a probabilistic version of rock-paper-scissors; everything is beaten by something.\n\n\n\nIf Player 1 picks…\nThen Player 2 should pick…\n\n\n\n\nHHH\nTHH\n\n\nHHT\nTHH\n\n\nHTH\nHHT\n\n\nHTT\nHHT\n\n\nTHH\nTTH\n\n\nTHT\nTTH\n\n\nTTH\nHTT\n\n\nTTT\nHTT\n\n\n\nThe easy way to remember what Player 2 should pick is: if Player 1 picks \\(123\\), then Player 2 should pick \\(\\overline 212\\), where \\(\\overline 2\\) denotes the opposite of \\(2\\).\nA logical consequence of this is that there are cycles of things that generally defeat each other. For instance:\n\nHHT usually loses to THH,\nwhich usually loses to TTH,\nwhich usually loses to HTT,\nwhich usually loses to HHT.\n\nIn other words, we have a naturally-occurring example of a nontransitive game.\n\n\n4. Not all sequences are created equal\nAlthough every sequence is beaten by something, not every sequence beats something else; TTT and HHH have no advantage over any other sequence. The best case scenario for those sequences is a fair game.\n\n\n5. Limits of simulations\nThis turns out to be a decent advertisement for the limits of simulations (as compared to theoretical approaches) as well. Generally, simulations are quite good at conveying big-picture ideas, like the takeaways we identified above. Yet, simulations are not as well-suited to identifying small differences between things. For instance, the theoretical win probability of HHH over THT is \\(5/12 \\approx 0.4167\\), and the theoretical win probability of HHH over HTT is \\(2/5 = 0.40\\). (See Further Reading for details.) Yet in the heatmap above, these two values are close enough that it is not obvious they represent different numbers under the hood. This can be solved with more replications, of course, but that is an expensive solution that scales poorly as numbers get closer together.\n\n\n\nTo be continued….\nSo far, we’ve seen that Penney’s Game is certainly not fair and that it contains a nontransitive game inside it. However, this isn’t the end of the story; even if we remove the competitive aspect of the game and just focus on what happens with one player, the game still doesn’t behave like you might expect.\n\n\nFurther Reading\nYou can find a theoretically-driven explanation of Penney’s Game, along with a chart of exact win probabilities (rather than approximations rendered from simulations), here."
  },
  {
    "objectID": "posts/2022-08-10-penneys-game-2/index.html",
    "href": "posts/2022-08-10-penneys-game-2/index.html",
    "title": "Penney’s Game, pt. 2",
    "section": "",
    "text": "Photo by Chris Briggs on Unsplash \n\n\n\n\nReimagining Penney’s Game\nWe’ll start with a modified version of Penney’s Game. As before, you’re playing a game involving a sequence of coin flips – but this time, you’re playing it alone. You have chosen a sequence of three coin flips (let’s say, HTH), and you’re waiting to see how long it will take to encounter your sequence.\nIf your sequence of coins was\n\nTHTTHTH\n\nthen it would have taken you 7 coin flips to find the sequence you wanted. Clearly, if you’re lucky, you might encounter your sequence in the first three flips; on the other hand, if you’re unlucky, it could take a hundred coin flips for you to see your sequence. The question is: does the choice of sequence affect how long it will take you to encounter it, on average?\n\n\nThe Incorrect Answer\nMuch like the first Penney’s Game question, a little knowledge is a dangerous thing. In a fixed collection of 3 coin flips, each sequence of length three has a \\(\\frac{1}{2^3} = \\frac 1 8\\) chance of occurring. When I first heard this question, it seemed like a pretty clear application of a geometric random variable, and since the expected wait time for a \\(\\operatorname{Geometric(p)}\\) random variable is \\(1/p\\), it made sense to me that each sequence might have the same wait time.\nAnd yet…\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\n\n\npenney_wait <- function(p1){        # p1 is a string like \"HHT\"\n  \n  n_coins <- nchar(p1)\n  p1 <- unlist(str_split(p1, \"\"))   # transform p1 into string vector\n    \n  # initialize first collection of coin flips\n  flips <- sample(c(\"H\", \"T\"), n_coins, replace = T) \n\n  num_flips <- n_coins              # keeps track of total number of flips\n    \n  # on each pass of the while loop, push the first n-1 coins forward in the list,\n  # then sample a new coin for the nth one; for instance, HTH becomes TH? where\n  # \"?\" is the result of a new coin flip\n  while(!all(p1 == flips)){\n    flips[1:(n_coins - 1)] <- flips[2:n_coins]\n    flips[n_coins] <- sample(c(\"H\", \"T\"), 1)\n    num_flips <- num_flips + 1\n  }\n    \n  num_flips                         # return number of flips needed to win\n}\n\n\n# The next function produces a vector of appropriate length enumerating all\n# possible sequences of coin flips of that length; for instance, ht_iterator(2)\n# will return the vector c(\"HH\", \"HT\", \"TH\", \"TT\"). This works recursively by \n# calling ht_iterator(k-1), duplicating it, and appending each of H and T to \n# one of the duplicates.\n\nht_iterator <- function(k){\n  if(k == 1) {\n    return(c(\"H\", \"T\"))\n  } else {\n    ht_iterator(k-1) %>%\n      rep(each = 2) %>%\n      paste0(c(\"H\", \"T\"))\n  } \n}\n\ntibble(sequence = ht_iterator(3)) %>%\n  rowwise() %>%\n  mutate(avg_wait = mean(replicate(1e4, penney_wait(sequence)))) %>%\n  ggplot(aes(x = sequence, y = avg_wait)) +\n    geom_bar(stat = \"identity\") +\n    labs(title = \"Monte Carlo simulations of wait times\",\n         subtitle = \"10,000 total trials per sequence\",\n         y = \"Average wait time\") +\n    theme(text = element_text(size = 20)) +\n    coord_flip()\n\n\n\n\nOf course, I wasn’t thinking through the fact that this clearly isn’t a geometric random variable of that structure, since we don’t need the sequences to fit inside predetermined blocks of three. In the above example, our sequence of coin flips was:\n\nTHT|THT|H..|\n\nAdopting the geometric perspective would be tantamount to requiring that the sequences occur strictly between the vertical bars – but that doesn’t need to happen here.\n\n\nThe Correct Answer\nAs we can see clearly from the graph, different sequences have different average wait times, and it follows that something about the non-geometricness of the process is the root cause. However, it may not be obvious at first what it is about being non-geometric would cause this discrepancy between sequences. As suggested by the graphs above, there are three tiers of expected wait times in the Penney’s Game problem with three coins:\n\n\n\ncoin\navg. wait time\n\n\n\n\nHHH, TTT\n14\n\n\nHTH, THT\n10\n\n\nHHT, HTT, THH, TTH\n8\n\n\n\n\n\nBut why?\nThe simulation clearly shows that there’s a difference in wait times, but it doesn’t do much to illuminate the reason for it. For that, we need to think carefully about what happens when we’re chasing a sequence and we don’t see what we are hoping for. We will illustrate this concept with some diagrams, starting with HHH.\n\nThe above diagram illustrates the collection of possible things that can occur when we’re trying to complete the HHH sequence. When we begin, we want to see three flips of heads in a row. Each successive flip of H moves us to the right along the diagram. But if at any point we miss and see a T, we lose all our progress and have to start over from scratch.\n\nThrough this perspective, we can start to see why HTH is a fundamentally friendlier sequence to chase than HHH. If we have obtained the first H in the sequence, we hope that the next throw will be a T; however, if it isn’t, then it’s an H instead. This means that when we’re at step 1 of 3 in completing the sequence, missing the coin we hope to see just means we remain at step 1 instead of resetting all the way back to the start.\n\nThe HHT case is even more favorable; here, once we see two consecutive H’s, there’s no way to lose our progress; we’re just waiting for the eventual trailing T.\nExamining these three cases carefully gives a useful perspective: the secret is in the blue backtracking arrows. Let’s recap what blue arrows we’ve seen in the diagrams above:\n\nHHH\n\none loop\ntwo blue arrows (one long, one short)\n\nHTH\n\ntwo loops\none long blue arrow\n\nHHT\n\ntwo loops\none short blue arrow\n\n\nThis shows us that HHH has the most backtracking, HTH has the second most, and HHT has the least, which corresponds to their respective average wait times. This should make sense; the wait time will be determined not by what happens when you get the coin throws you want, but by how far your progress is set back when you don’t get what you want.\n(NB: Hopefully, this argument makes it clear why some wait times are longer than others; however, it falls short of establishing that those wait times are exactly 8, 10, and 14. I’ll save those details for another post.)\n\n\nTo be continued….\nIn the first post on Penney’s Game, we saw that different sequences have various advantages over each other in a head-to-head matchup; now, we’ve also seen that different sequences have different expected wait times in isolation. To me, these two things are already plenty strange – and yet, somehow, things will get even more bizarre when we put a fourth coin in the mix."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Coming Soon….",
    "section": "",
    "text": "Please excuse my dust! I’m in the middle of rebuilding my old blog. You win three internet points for finding this temporary page!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]