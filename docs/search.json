[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I’m Aaron Montgomery, an Associate Professor at Baldwin Wallace University. I received a PhD in Mathematics (specializing in Probability Theory) in 2013 at the University of Oregon under the direction of David Levin. Since then, I have been a member of the BW Mathematics and Statistics faculty, and more recently I have become the coordinator of the new Data Science and Data Analytics programs.\nI am always open to collaborating on interesting statistical or data-oriented projects! You can reach me at amontgom (atsymbol) bw.edu. You can find the GitHub source for this blog here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aaron M. Montgomery",
    "section": "",
    "text": "This blog is an assorted collection of notes on probability, statistics, data, and coding.\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nUniform Products: Problem\n\n\n\n\n\n\n\ncounterintuitive\n\n\npython-language\n\n\n\n\nA pocket-sized probability puzzle\n\n\n\n\n\n\nApr 12, 2023\n\n\n\n\n\n\n  \n\n\n\n\nUniform Products: Solution\n\n\n\n\n\n\n\ncounterintuitive\n\n\npython-language\n\n\nsimulation\n\n\n\n\n(Read the Problem post first!)\n\n\n\n\n\n\nApr 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nChatGPT: A Calculator for the Humanities?\n\n\n\n\n\n\n\nteaching\n\n\nai\n\n\nchatgpt\n\n\n\n\nExamining the role of AI tools in the classroom\n\n\n\n\n\n\nJan 30, 2023\n\n\n\n\n\n\n  \n\n\n\n\nTeaching Data 101 with Dataquest\n\n\n\n\n\n\n\ndataquest\n\n\nteaching\n\n\ndata-101\n\n\n\n\nLessons from the first offering of the class\n\n\n\n\n\n\nJan 6, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPenney’s Game, pt. 3\n\n\n\n\n\n\n\nsimulation\n\n\npenneys-game\n\n\ncounterintuitive\n\n\nr-language\n\n\n\n\nThe graveyard of reasonable conjectures\n\n\n\n\n\n\nSep 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\nPenney’s Game, pt. 2\n\n\n\n\n\n\n\nsimulation\n\n\npenneys-game\n\n\ncounterintuitive\n\n\nr-language\n\n\n\n\nThe waiting (time) is the hardest part\n\n\n\n\n\n\nAug 10, 2022\n\n\n\n\n\n\n  \n\n\n\n\nPenney’s Game, pt. 1\n\n\n\n\n\n\n\nsimulation\n\n\npenneys-game\n\n\ncounterintuitive\n\n\nr-language\n\n\n\n\nA naturally-occurring rock-paper-scissors\n\n\n\n\n\n\nAug 5, 2022\n\n\n\n\n\n\n  \n\n\n\n\nHey, Who Invited You?\n\n\n\n\n\n\n\nsimulation\n\n\neulers-number\n\n\ncounterintuitive\n\n\nr-language\n\n\n\n\nA coding problem brings an uninvited famous mathematical guest\n\n\n\n\n\n\nJul 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\nLet’s Make a Deal!\n\n\n\n\n\n\n\nsimulation\n\n\nmonte-carlo\n\n\ncounterintuitive\n\n\nr-language\n\n\n\n\nMonty Hall meets Monte Carlo\n\n\n\n\n\n\nJul 14, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-07-10-monty-hall/index.html",
    "href": "posts/2022-07-10-monty-hall/index.html",
    "title": "Let’s Make a Deal!",
    "section": "",
    "text": "Photo by Sebastian Herrmann on Unsplash"
  },
  {
    "objectID": "posts/2022-07-10-monty-hall/index.html#the-incorrect-answer",
    "href": "posts/2022-07-10-monty-hall/index.html#the-incorrect-answer",
    "title": "Let’s Make a Deal!",
    "section": "The Incorrect Answer",
    "text": "The Incorrect Answer\nI was first exposed to this problem in college when a friend posed the question to me as a brain teaser to at a cafeteria table. I gave the only sensible answer, which was to say that since you had two doors, getting the prize was a 50/50 proposition no matter what you chose. I was, of course, dead wrong.\nFor years, I was slightly embarrassed about this. Not too embarrassed, mind you – after all, I’m wrong all the time. But this one stung because my friend was on my home turf; at this point in my life, I was pretty sure I was going to try to pursue a PhD in Mathematics and I suspected I might gravitate to the field of Probability Theory. Just like that, I whiffed on a probability question in a semi-public forum.\nMy shame was lessened over the years when I learned that if nothing else, I wasn’t alone. When Marilyn Vos Savant gave a correct solution to the problem in a 1990 issue of Parade, she received a truckload of letters, many from professional mathematicians, telling her how wrong she was. (She wasn’t wrong, which certainly didn’t help the outrageous rudeness of some of those letters.) In his book Which Door Has the Cadillac: Adventures of a Real Life Mathematician, Andrew Vazsonyi recalls giving the same incorrect “obvious” answer to the problem on his first encounter; perhaps more shockingly, he details an account of discussing the problem with Paul Erdös, who also got the problem wrong and became increasingly irate about it until he was eventually shown a simulation proving what the right answer should be."
  },
  {
    "objectID": "posts/2022-07-10-monty-hall/index.html#the-correct-answer",
    "href": "posts/2022-07-10-monty-hall/index.html#the-correct-answer",
    "title": "Let’s Make a Deal!",
    "section": "The Correct Answer",
    "text": "The Correct Answer\nThat right answer is that switching is better. Indeed, staying with your original choice will grant you a \\(1/3\\) chance of winning, and switching will grant a \\(2/3\\) chance of winning. The key detail, of course, is Monty’s knowledge of the prize location and his choice of exactly how to reveal what he knows. There are many ways to see why this is true; the Wikipedia entry for the Monty Hall problem gives many different explanations of many different flavors (and even criticisms of those same explanations). These explanations are great, but to those who aren’t accustomed to long mathematical arguments, they might be less than convincing.\nThe first explanation of the solution in the Wikipedia article states:\n\nWhen the player first makes their choice, there is a \\(2/3\\) chance that the car is behind one of the doors not chosen. This probability does not change after the host reveals a goat behind one of the unchosen doors.\n\nThis explanation never quite sat right with me. Sasha Volokh expressed my vague concern quite well:\n\nFirst, it’s clear that any explanation that says something like “the probability of door 1 was 1/3, and nothing can change that…” is automatically fishy: probabilities are expressions of our ignorance about the world, and new information can change the extent of our ignorance.\n\nThis is a case where simulations can do us some good."
  },
  {
    "objectID": "posts/2022-07-10-monty-hall/index.html#simulations",
    "href": "posts/2022-07-10-monty-hall/index.html#simulations",
    "title": "Let’s Make a Deal!",
    "section": "Simulations",
    "text": "Simulations\nWe’ll write Monte Carlo simulations in R to see that by sticking with our original answer, the probability of winning is indeed \\(1/3\\). We will write a function that simulates one full round of the game; then, we’ll replicate() the function many times to determine the probability of winning. Our strategy will be to simulate a round of the full game many times and keep track of how often the game results in a win.\nFor our first attempt, we’ll recreate a very general version of the game:\n\nthe prize can be found behind any of 3 doors\nthe contestant will pick any of 3 doors\nMonty will reveal one door and offer a chance to switch\nthe contestant will choose to switch or stay depending on the parameter stay\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\nmonty_hall <- function(stay){\n    \n  doors <- c(\"A\", \"B\", \"C\")              # using ABC due to a quirk in sample()\n\n  prize_door <- sample(doors, 1)\n  contestant_choice <- sample(doors, 1)\n    \n  reveal_door <-                                # Monty can't reveal door with the prize,\n    union(prize_door, contestant_choice) %>%    # nor the selected door, so...\n    setdiff(doors, .) %>%                       # we remove those choices \n    sample(1)  \n\n  switch_offer <- setdiff(doors, c(contestant_choice, reveal_door))\n    \n  ifelse(prize_door == ifelse(stay, contestant_choice, switch_offer),\n         \"win\", \"lose\")\n    # the function returns the strings \"win\" and \"lose\"\n    # when stay is TRUE, check if the prize door matches the contestant's choice\n    # when stay is FALSE, check if the prize door matches the door offered in a switch\n}\n\nNext, we’ll generate 10K trials of the game under each of the two options (switching and staying). We’ll reshape the outcomes just a bit and then plot them.\n\nmonty_stay_trials <- replicate(10000, monty_hall(stay = TRUE))\nmonty_switch_trials <- replicate(10000, monty_hall(stay = FALSE))\n\ndata.frame(stay = monty_stay_trials, switch = monty_switch_trials) %>%\n  pivot_longer(everything(), names_to = \"choice\", values_to = \"outcome\") %>% \n  ggplot(aes(x = outcome)) +\n    geom_bar(stat = \"count\") +\n    facet_wrap(~choice) +\n    labs(title = \"Monty Hall Monte Carlo simulation results\",\n         subtitle = \"10,000 trials per contestent choice possibility\") +\n    theme(text = element_text(size = 20))\n\n\n\n\nThe simulations show us exactly what we expected; switching is good, staying is bad. We can also easily confirm that the probability of winning by switching is \\(2/3\\):\n\nmean(monty_switch_trials == \"win\")\n\n[1] 0.6657\n\n\nOur answer is close to \\(2/3\\), and the difference between it and \\(2/3\\) is a small statistical fluctuation, as expected.\nSo, good news: we have confirmed that the correct answer is indeed correct. But can we render any deeper insights from this? Let’s focus on the function in the case when stay == TRUE. In that case, the code looks like this:\nmonty_hall <- function(stay){\n    \n  doors <- c(\"A\", \"B\", \"C\")      \n\n  prize_door <- sample(doors, 1)\n  contestant_choice <- sample(doors, 1)\n    \n  reveal_door <- \n    union(prize_door, contestant_choice) %>%    \n    setdiff(doors, .) %>%                       \n    sample(1)  \n\n  switch_offer <- setdiff(doors, c(contestant_choice, reveal_door))\n    \n  ifelse(prize_door == contestant_choice, \"win\", \"lose\")\n}\n\nHere, switch_offer doesn’t actually get used at all in the line that returns the function. This means that the switch_offer <- ... line, and the reveal_door <- ... lines, are unused appendages. If we remove them, we’re left with this:\nmonty_hall <- function(stay){\n    \n  doors <- c(\"A\", \"B\", \"C\")      \n\n  prize_door <- sample(doors, 1)\n  contestant_choice <- sample(doors, 1)\n    \n  ifelse(prize_door == contestant_choice, \"win\", \"lose\")\n}\nNow, we see that our code has reduced to a simulation that draws two objects independently from a collection of three and checks to see if they’re the same. That probability is clearly \\(1/3\\), and this explanation now aligns perfectly with the Wikipedia explanation that nothing has changed the original probability of \\(1/3\\), whether that explanation is “fishy” or not."
  },
  {
    "objectID": "posts/2022-07-10-monty-hall/index.html#takeaway",
    "href": "posts/2022-07-10-monty-hall/index.html#takeaway",
    "title": "Let’s Make a Deal!",
    "section": "Takeaway",
    "text": "Takeaway\nIt’s great to be able to use Monte Carlo simulations to confirm a correct answer, but in this case the act of writing a simulation can do something more profound: it can make the why behind the answer just a bit more convincing. Or, at least, it did that for me. I’ve seen (and believed, and produced) many analytical arguments for why switching has a \\(2/3\\) probability, but I never fully believed the Wikipedia explanation until writing code to simulate the game."
  },
  {
    "objectID": "posts/2022-07-27-ordered-objects/index.html",
    "href": "posts/2022-07-27-ordered-objects/index.html",
    "title": "Hey, Who Invited You?",
    "section": "",
    "text": "Photo by Surendran MP on Unsplash"
  },
  {
    "objectID": "posts/2022-07-27-ordered-objects/index.html#the-incorrect-answer",
    "href": "posts/2022-07-27-ordered-objects/index.html#the-incorrect-answer",
    "title": "Hey, Who Invited You?",
    "section": "The Incorrect Answer",
    "text": "The Incorrect Answer\nMy immediate answer was: No, that must just be a coincidence."
  },
  {
    "objectID": "posts/2022-07-27-ordered-objects/index.html#the-correct-answer",
    "href": "posts/2022-07-27-ordered-objects/index.html#the-correct-answer",
    "title": "Hey, Who Invited You?",
    "section": "The Correct Answer",
    "text": "The Correct Answer\nI was wrong. The true answer was actually \\(e\\). Well, almost.\n(Quick disclaimer: this result is perfectly well-known to humanity; it just wasn’t well-known to me at the time.)\nWhen the student asked me this question, I replicated my answer a few more times to see if I still thought the resemblence to the magic number \\(2.718282...\\) was accidental.\n\nmean(replicate(10000, letters_until_unordered()))\n\n[1] 2.7228\n\n\nHmmm…\n\nmean(replicate(10000, letters_until_unordered()))\n\n[1] 2.726\n\n\nVery suspicious. Let’s try using more replications for finer accuracy:\n\nmean(replicate(1e5, letters_until_unordered()))\n\n[1] 2.71936\n\n\nHMMMM…..\n\nmean(replicate(1e6, letters_until_unordered())) \n\n[1] 2.717368\n\n# runtime starts to get a little long: ~50 seconds on my machine\n\nThat was hard to ignore, and it practically reeked of Euler’s number. This caught me completely off guard; I had written the problem without any idea that \\(e\\) belonged here at all, and yet… here it was. On the one hand, I shouldn’t have been surprised at all. This is just a Thing That Happens with certain famous mathematical constants like \\(e\\) and \\(\\pi\\); they frequently show up where they frankly have no business being.\nI did find myself surprised, though – mostly, by the fact that I had encountered an experiment-driven result, which I am not at all accustomed to. I don’t think I would ever have considered this result were it not for this esoteric homework problem I conjured up (and the help of the keen-eyed student). Monte Carlo simulations gave me something that I imagine must feel a little bit like when a physicist can’t explain a bump on a curve, or when a biologist sees a bacterial growth rate that has no known explanation. The next step for that physicist, or that biologist, and for me, was to ask why.\nIn this way, I was behaving in the most analogous way possible to an experimental scientist. Such a scientist might discover a fascinating new result in a lab; new results yearn for new theories, and it is those theories that are powerful. Theories are what convey existing and lasting knowledge to humanity. The experiments themselves matter only insofar as they suggest or reveal deeper truths; but the experiments themselves are but partial reflections of those truths. This homework problem doesn’t matter; what makes it tick matters.\nThis also helped crystallize to me an important lesson about the pedagogy of mathematical education. One of my most important missions in my own classrooms has been to transfer as much agency to students as possible. I tend to gravitate strongly toward active learning strategies. My teaching philosophy is fairly simple: anything students do is good, and anything I do is inherently less good. Here, too, I’ve long been jealous of lab sciences; they have a built-in pedagogy to invite students to meaningfully engage, right from the very beginning of study. Teaching a course on Monte Carlo simulations has been the first thing I’ve done that has felt just a little bit like a lab.\nAnd yet, the simulations have also underscored for me the importance of understanding “traditional” mathematics; we can discover wildly interesting results through Monte Carlo simulations, but only mathematical reasoning can deliver the all-important why. Simulations are a wonderful way to augment a traditional probability and statistics curriculum, but I don’t think they should replace it."
  },
  {
    "objectID": "posts/2022-07-27-ordered-objects/index.html#takeaway",
    "href": "posts/2022-07-27-ordered-objects/index.html#takeaway",
    "title": "Hey, Who Invited You?",
    "section": "Takeaway",
    "text": "Takeaway\nMonte Carlo simulations have given me a new way to invite students who can write just a bit of code to experiment and to play in a mathematical space. Inviting students into this style of mathematical “laboratory” has been incredibly rewarding, and it has allowed me to engage students much more fully than I’m typically able to do in introductory probability / statistics classes. It has allowed me to transfer some of the ownership of the subject material to students, which seems to me an unqualified good thing.\nHowever, Monte Carlo simulations are inherently limited in scope. They can give us incredibly interesting results, and they can do so in a much more accessible way than can classical mathematics and statistics. But by themselves, they can never give us that elusive why."
  },
  {
    "objectID": "posts/2022-07-27-ordered-objects/index.html#wait-what-about-the-why-part",
    "href": "posts/2022-07-27-ordered-objects/index.html#wait-what-about-the-why-part",
    "title": "Hey, Who Invited You?",
    "section": "Wait, what about the “Why” part?",
    "text": "Wait, what about the “Why” part?\nIt would be pretty unsatisfying if after all that discussion, we didn’t get around to the proof of the aforementioned result. Once again, to be clear, this result is perfectly well-known; I just hadn’t personally encountered it before. Once the student brought this seeming coincidence to my attention, I immediately set out to try to prove it.\nTo remind ourselves, here’s the result:\n\nTheorem\nSuppose you repeatedly draw upper-case letters from a jar without replacement until the first time you get one that is out of order. The expected number of draws executed in this fashion will be very slightly less than \\(e\\).\n\n\nProof\nFor \\(k \\geq 1\\), let \\(A_k\\) denote the event that the first \\(k\\) letters drawn are all in order. (That is: \\(A_1\\) is trivial, \\(A_2\\) holds only if the first two letters are drawn in order, and so forth.) If we let \\(N\\) denote random variable of the number of letters drawn until finding one out of sequence, then we note that \\[ N = 1 + \\mathbb 1_{A_1} + \\mathbb 1_{A_2} + \\dots + \\mathbb 1_{A_{26}}\\] where \\(\\mathbb 1_{B}\\) denotes the indicator function of event \\(B\\). This is both the most important and least obvious step in the proof; the logic of this step is that \\(N\\) and each \\(\\mathbb 1_{A_k}\\) are random variables, meaning they’re functions of some unseen input variable \\(\\omega\\). Here, choosing an \\(\\omega\\) determines a full ordering of the letters, and once this has been done, a certain number of events \\(\\{A_1, \\dots, A_n\\}\\) will be fulfilled, making their corresponding indicator variables evaluate to \\(1\\) on that \\(\\omega\\). Summing these indicators will therefore count the events, as desired.\nThe leading \\(1\\) in the above expression is meant to count the one letter we’ll draw that’s out of order and therefore not counted by the indicator variables; technically, this isn’t very well-defined for the case when we draw all 26 letters in order, but that’s a very low-probability event that we were explicitly invited in the problem statement to ignore, so we’ll just accept the fact that \\(N\\) evaluates to \\(27\\) in that case. (I originally added that detail in the problem statement to alleviate a coding nuisance, and it turned out to alleviate a corresponding mathematical nuisance here.)\nThe decomposition above is useful, because it therefore follows that \\[ \\mathbb E[N] = 1 + \\mathbb E \\left[ \\mathbb 1_{A_1} \\right] + \\mathbb E \\left[ \\mathbb 1_{A_2} \\right] + \\dots + \\mathbb E \\left[ \\mathbb 1_{A_{26}} \\right] = 1 + \\sum_{k=1}^{26} \\mathbb E \\left[ \\mathbb 1_{A_k} \\right].\\] I should stop and explicitly acknowledge that this really, really feels like we’re cheating somehow. The \\(\\{A_k\\}\\) events are definitely not independent; in fact, they’re completely dependent, as \\(A_{k+1} \\subset A_k\\). (If the first twelve letters were perfectly ordered, then the first eleven were also.) However, independence is not required for linearity of expectation.\nFrom here, we recall that \\(\\mathbb E \\left[ \\mathbb 1_B \\right] = \\mathbb P(B)\\) for any event \\(B\\), which means that we need only to compute \\(\\mathbb P(A_k)\\). When drawing \\(k\\) objects, there are \\(k!\\) rearrangements of those objects, of which only \\(1\\) is properly ordered, from which it follows that \\(\\mathbb P(A_k) = \\frac 1 {k!}\\). Hence, \\[\\mathbb E[N] = 1 + \\sum_{k=1}^{26} \\mathbb P \\left( A_k \\right) = 1 + \\sum_{k=1}^{26} \\frac{1}{k!} = \\sum_{k=0}^{26} \\frac{1}{k!}\\] and we notice that this is simply the first few terms of the infinite series \\[\\sum_{k=0}^{\\infty} \\frac{1}{k!} = e.\\]\nSo, the true expected value in our original question isn’t quite \\(e\\) – it’s a number just below it. Specifically, it’s \\(e - \\sum_{k=27}^{\\infty} \\frac{1}{k!}\\). To see how close this is to \\(e\\), let’s use R to sum the first few terms of the remainder series:\n\nsum(1 / factorial(27:1000))\n\n[1] 9.523378e-29\n\n\nPretty close to \\(e\\), indeed. And while the true answer isn’t actually quite \\(e\\), we were never going to be able to distinguish the true answer from \\(e\\) with only a Monte Carlo simulation."
  },
  {
    "objectID": "posts/2022-08-05-penneys-game-1/index.html",
    "href": "posts/2022-08-05-penneys-game-1/index.html",
    "title": "Penney’s Game, pt. 1",
    "section": "",
    "text": "Photo by Dan Dennis on Unsplash \n\n\n\n\nPenney’s Game\nThe setup: you’re playing a game with a friend involving coin flips. Both you and your friend choose a sequence of three coin flips; let’s say that you choose THH, and your friend chooses HHT. A neutral referee flips a single coin repeatedly until one of you encounters the sequence you’re waiting for. So, for instance, if the sequence of coin flips is\n\nTHTTHTHTHH\n\nthen you would win the game, because THH occurred before HHT ever did. The question is: is this a fair game? Or, does one player have an advantage over the other?\n\n\nThe Incorrect Answer\nThis question, introduced by Walter Penney (hence the name), is a great question to trip up some people who know some probability. When I first heard this question, the answer seemed obvious – of course it’s fair! After all, I knew that in a sequence of of three isolated coin flips, both THH and HHT (and any other sequence of coin flips) had a \\(\\frac 1 2 \\cdot \\frac 1 2 \\cdot \\frac 1 2 = \\frac 1 8\\) chance of appearing. That claim is of course true, but in the spirit of a Mathematical Laboratory, let’s take the opportunity to visualize it with some quick Monte Carlo simulations:\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\n\n\n# the first line makes a vector of 10,000 strings like \"HTH\" or \"TTT\"\n\nreplicate(1e5, paste0(sample(c(\"H\", \"T\"), 3, replace = T), collapse = \"\")) %>%\n  tibble(sequence = .) %>%\n  arrange(sequence) %>%\n  ggplot(aes(x = sequence)) +\n    geom_bar(stat = \"count\") +\n    labs(title = \"Monte Carlo simulations of flipping 3 coins\",\n         subtitle = \"10,000 total trials\") +\n    theme(text = element_text(size = 20))\n\n\n\n\nAs expected, we encounter each of the 8 distinct sequences with approximately equal frequency, so we haven’t gone astray yet. The problem arises when we try to extend that logic to the actual game described up above; in this case, we’re waiting for one of two sequences to appear instead of just stopping after 3 flips and logging the outcomes. Here are the simulations for the THH vs. HHT game we originally described:\n\n# This function will take two strings of coin flips and return the one that occurs\n# first in a long sequence of coin flips.\n\npenney <- function(p1, p2){         # p1 and p2 are strings like \"HHT\", \"HTT\"\n  p1 <- unlist(str_split(p1, \"\"))   # split string into vector of length 3\n  p2 <- unlist(str_split(p2, \"\"))\n  flips <- sample(c(\"H\", \"T\"), 3, replace = T) \n    # initialize flips with first 3 coin flips\n    \n    # on each pass of the while loop, push the first two coins forward in the list,\n    # then sample a new coin for the third one; for instance, HTH becomes TH? where\n    # ? is the result of a new coin flip\n    \n  while(!all(p1 == flips) & !all(p2 == flips)){\n    flips[1:2] <- flips[2:3]\n    flips[3] <- sample(c(\"H\", \"T\"), 1)\n  }\n    \n  ifelse(all(p1 == flips), \n         paste0(p1, collapse = \"\"), \n         paste0(p2, collapse = \"\"))  # output winning sequence\n}\n\n\nreplicate(10000, penney(\"HHT\", \"THH\")) %>%\n  tibble(winner = .) %>%\n  arrange(winner) %>%\n  ggplot(aes(x = winner)) +\n    geom_bar(stat = \"count\") +\n    labs(title = \"Monte Carlo simulations of Penney's Game\",\n         subtitle = \"10,000 total trials: HHT vs. THH\") +\n    theme(text = element_text(size = 20))\n\n\n\n\nIt is immediately clear that this is not a fair game.\n\n\nThe Correct Answer\nOnce we see that the game is not fair, we should ask why. For these particular two coin flips, there’s a good argument of why HHT should beat THH just one out of four times: the only way that HHT can win is if the first two coin flips are both heads. Otherwise, there’s some first occurrence of two H’s in the sequence, and that first occurrence is necessarily preceded by a T. The probability that this occurs is \\(\\frac 1 2 \\cdot \\frac 1 2 = \\frac 1 4\\), which is why HHT won about 2500 times out of our 10,000 simulations above.\nAfter we understand this head-to-head matchup, the obvious question is: what about all possible head-to-head matchups? To see, we’ll return to the “laboratory” by simulating every possible matchup of all three sequences and generating a heatmap of the results.\n\n# this helper function takes two coin sequences, simulates Penney's game many\n# times, and returns the proportion of times the *first* sequence wins\npenney_wrapper <- function(p1, p2){\n  mean(replicate(1e4, penney(p1, p2)) == p1)\n}\n\n# make a data frame with all combinations of sequences\nresults <-\n  c(\"HHH\", \"HHT\", \"HTH\", \"HTT\", \"THH\", \"THT\", \"TTH\", \"TTT\") %>%\n  combn(2) %>%\n  t() %>%\n  as.data.frame()\n\nnames(results) <- c(\"p1\", \"p2\")\n\nresults <- \n  results %>%\n  rowwise() %>%\n  mutate(p1_wins = penney_wrapper(p1, p2)) %>%\n  mutate(p1_wins = round(p1_wins, 2))\n\n# since we simulated only half of the matchups, we can just mirror\n# the results to flip p1 and p2 rather than resimulating\nresults <-\n  tibble(\n    p1 = results$p2,\n    p2 = results$p1,\n    p1_wins = 1 - results$p1_wins\n  ) %>%\n    bind_rows(results)\n\nresults %>%\n  ggplot(aes(x = p1, y = p2, fill = p1_wins)) +\n    geom_tile() +\n    geom_text(aes(label = p1_wins), size = 6) +\n    scale_fill_gradient2(low = \"yellow\", mid = \"white\", \n                         high = \"cyan\", midpoint = 0.5) +\n    labs(title = \"Head-to-head matchups: Penney's Game\",\n         subtitle = \"10K trials per matchup combination\",\n         fill = \"p1 win %\") +\n    theme(text = element_text(size = 20))\n\n\n\n\nThis chart reveals some pretty fascinating truths about Penney’s Game. Here are some of the things we learn:\n\n1. On the whole, Penney’s Game is certainly not fair.\nWe can see that some matchups have… * slight advantages, like THH over TTT * moderate advantages, like HHT over TTT * overwhelming advantages, like HTT over TTT\n\n\n2. Yet, fair games do exist within Penney’s Game.\nNo matter which sequence Player 1 picks, Player 2 can pick something that has a \\(50\\%\\) chance of beating it; one way to do this is to take the inverse of Player 1’s sequence, like picking TTH in response to HHT.\n\n\n3. Rock-Paper-Scissors\nOn the other hand, if Player 1 picks their sequence first, then no matter what they have picked, Player 1 can pick something that has an advantage over it. This is almost a probabilistic version of rock-paper-scissors; everything is beaten by something.\n\n\n\nIf Player 1 picks…\nThen Player 2 should pick…\n\n\n\n\nHHH\nTHH\n\n\nHHT\nTHH\n\n\nHTH\nHHT\n\n\nHTT\nHHT\n\n\nTHH\nTTH\n\n\nTHT\nTTH\n\n\nTTH\nHTT\n\n\nTTT\nHTT\n\n\n\nThe easy way to remember what Player 2 should pick is: if Player 1 picks \\(123\\), then Player 2 should pick \\(\\overline 212\\), where \\(\\overline 2\\) denotes the opposite of \\(2\\).\nA logical consequence of this is that there are cycles of things that generally defeat each other. For instance:\n\nHHT usually loses to THH,\nwhich usually loses to TTH,\nwhich usually loses to HTT,\nwhich usually loses to HHT.\n\nIn other words, we have a naturally-occurring example of a nontransitive game.\n\n\n4. Not all sequences are created equal\nAlthough every sequence is beaten by something, not every sequence beats something else; TTT and HHH have no advantage over any other sequence. The best case scenario for those sequences is a fair game.\n\n\n5. Limits of simulations\nThis turns out to be a decent advertisement for the limits of simulations (as compared to theoretical approaches) as well. Generally, simulations are quite good at conveying big-picture ideas, like the takeaways we identified above. Yet, simulations are not as well-suited to identifying small differences between things. For instance, the theoretical win probability of HHH over THT is \\(5/12 \\approx 0.4167\\), and the theoretical win probability of HHH over HTT is \\(2/5 = 0.40\\). (See Further Reading for details.) Yet in the heatmap above, these two values are close enough that it is not obvious they represent different numbers under the hood. This can be solved with more replications, of course, but that is an expensive solution that scales poorly as numbers get closer together.\n\n\n\nTo be continued….\nSo far, we’ve seen that Penney’s Game is certainly not fair and that it contains a nontransitive game inside it. However, this isn’t the end of the story; even if we remove the competitive aspect of the game and just focus on what happens with one player, the game still doesn’t behave like you might expect.\n\n\nFurther Reading\nYou can find a theoretically-driven explanation of Penney’s Game, along with a chart of exact win probabilities (rather than approximations rendered from simulations), here."
  },
  {
    "objectID": "posts/2022-08-10-penneys-game-2/index.html",
    "href": "posts/2022-08-10-penneys-game-2/index.html",
    "title": "Penney’s Game, pt. 2",
    "section": "",
    "text": "Photo by Chris Briggs on Unsplash \n\n\n\n\nReimagining Penney’s Game\nWe’ll start with a modified version of Penney’s Game. As before, you’re playing a game involving a sequence of coin flips – but this time, you’re playing it alone. You have chosen a sequence of three coin flips (let’s say, HTH), and you’re waiting to see how long it will take to encounter your sequence.\nIf your sequence of coins was\n\nTHTTHTH\n\nthen it would have taken you 7 coin flips to find the sequence you wanted. Clearly, if you’re lucky, you might encounter your sequence in the first three flips; on the other hand, if you’re unlucky, it could take a hundred coin flips for you to see your sequence. The question is: does the choice of sequence affect how long it will take you to encounter it, on average?\n\n\nThe Incorrect Answer\nMuch like the first Penney’s Game question, a little knowledge is a dangerous thing. In a fixed collection of 3 coin flips, each sequence of length three has a \\(\\frac{1}{2^3} = \\frac 1 8\\) chance of occurring. When I first heard this question, it seemed like a pretty clear application of a geometric random variable, and since the expected wait time for a \\(\\operatorname{Geometric(p)}\\) random variable is \\(1/p\\), it made sense to me that each sequence might have the same wait time.\nAnd yet…\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\n\n\npenney_wait <- function(p1){        # p1 is a string like \"HHT\"\n  \n  n_coins <- nchar(p1)\n  p1 <- unlist(str_split(p1, \"\"))   # transform p1 into string vector\n    \n  # initialize first collection of coin flips\n  flips <- sample(c(\"H\", \"T\"), n_coins, replace = T) \n\n  num_flips <- n_coins              # keeps track of total number of flips\n    \n  # on each pass of the while loop, push the first n-1 coins forward in the list,\n  # then sample a new coin for the nth one; for instance, HTH becomes TH? where\n  # \"?\" is the result of a new coin flip\n  while(!all(p1 == flips)){\n    flips[1:(n_coins - 1)] <- flips[2:n_coins]\n    flips[n_coins] <- sample(c(\"H\", \"T\"), 1)\n    num_flips <- num_flips + 1\n  }\n    \n  num_flips                         # return number of flips needed to win\n}\n\n\n# The next function produces a vector of appropriate length enumerating all\n# possible sequences of coin flips of that length; for instance, ht_iterator(2)\n# will return the vector c(\"HH\", \"HT\", \"TH\", \"TT\"). This works recursively by \n# calling ht_iterator(k-1), duplicating it, and appending each of H and T to \n# one of the duplicates.\n\nht_iterator <- function(k){\n  if(k == 1) {\n    return(c(\"H\", \"T\"))\n  } else {\n    ht_iterator(k-1) %>%\n      rep(each = 2) %>%\n      paste0(c(\"H\", \"T\"))\n  } \n}\n\ntibble(sequence = ht_iterator(3)) %>%\n  rowwise() %>%\n  mutate(avg_wait = mean(replicate(1e4, penney_wait(sequence)))) %>%\n  ggplot(aes(x = sequence, y = avg_wait)) +\n    geom_bar(stat = \"identity\") +\n    labs(title = \"Monte Carlo simulations of wait times\",\n         subtitle = \"10,000 total trials per sequence\",\n         y = \"Average wait time\") +\n    theme(text = element_text(size = 20)) +\n    coord_flip()\n\n\n\n\nOf course, I wasn’t thinking through the fact that this clearly isn’t a geometric random variable of that structure, since we don’t need the sequences to fit inside predetermined blocks of three. In the above example, our sequence of coin flips was:\n\nTHT|THT|H..|\n\nAdopting the geometric perspective would be tantamount to requiring that the sequences occur strictly between the vertical bars – but that doesn’t need to happen here.\n\n\nThe Correct Answer\nAs we can see clearly from the graph, different sequences have different average wait times, and it follows that something about the non-geometricness of the process is the root cause. However, it may not be obvious at first what it is about being non-geometric would cause this discrepancy between sequences. As suggested by the graphs above, there are three tiers of expected wait times in the Penney’s Game problem with three coins:\n\n\n\ncoin\navg. wait time\n\n\n\n\nHHH, TTT\n14\n\n\nHTH, THT\n10\n\n\nHHT, HTT, THH, TTH\n8\n\n\n\n\n\nBut why?\nThe simulation clearly shows that there’s a difference in wait times, but it doesn’t do much to illuminate the reason for it. For that, we need to think carefully about what happens when we’re chasing a sequence and we don’t see what we are hoping for. We will illustrate this concept with some diagrams, starting with HHH.\n\nThe above diagram illustrates the collection of possible things that can occur when we’re trying to complete the HHH sequence. When we begin, we want to see three flips of heads in a row. Each successive flip of H moves us to the right along the diagram. But if at any point we miss and see a T, we lose all our progress and have to start over from scratch.\n\nThrough this perspective, we can start to see why HTH is a fundamentally friendlier sequence to chase than HHH. If we have obtained the first H in the sequence, we hope that the next throw will be a T; however, if it isn’t, then it’s an H instead. This means that when we’re at step 1 of 3 in completing the sequence, missing the coin we hope to see just means we remain at step 1 instead of resetting all the way back to the start.\n\nThe HHT case is even more favorable; here, once we see two consecutive H’s, there’s no way to lose our progress; we’re just waiting for the eventual trailing T.\nExamining these three cases carefully gives a useful perspective: the secret is in the blue backtracking arrows. Let’s recap what blue arrows we’ve seen in the diagrams above:\n\nHHH\n\none loop\ntwo blue arrows (one long, one short)\n\nHTH\n\ntwo loops\none long blue arrow\n\nHHT\n\ntwo loops\none short blue arrow\n\n\nThis shows us that HHH has the most backtracking, HTH has the second most, and HHT has the least, which corresponds to their respective average wait times. This should make sense; the wait time will be determined not by what happens when you get the coin throws you want, but by how far your progress is set back when you don’t get what you want.\n(NB: Hopefully, this argument makes it clear why some wait times are longer than others; however, it falls short of establishing that those wait times are exactly 8, 10, and 14. I’ll save those details for another post.)\n\n\nTo be continued….\nIn the first post on Penney’s Game, we saw that different sequences have various advantages over each other in a head-to-head matchup; now, we’ve also seen that different sequences have different expected wait times in isolation. To me, these two things are already plenty strange – and yet, somehow, things will get even more bizarre when we put a fourth coin in the mix."
  },
  {
    "objectID": "posts/2022-09-23-penneys-game-3/index.html",
    "href": "posts/2022-09-23-penneys-game-3/index.html",
    "title": "Penney’s Game, pt. 3",
    "section": "",
    "text": "Photo by Marcel Strauß on Unsplash \n\n\n\n\nThe Story So Far\nWe’ll begin with a quick recap.\nPenney’s Game, loosely, is a game about waiting for particular sequences of coin throws inside a larger sequence of fair coin throws. In our first foray into the topic, we examined head-to-head matchups between sequences of three coins. That is, you and a friend both choose a sequence of three coin flips, and the winner is determined by whose sequence appears first.\nThe first surprising result about this game – or at least, surprising if your intuition is anything like mine was – is that this game is far from fair, and that the win probabilities depend highly on which sequences are chosen.\n(NB: I’ve collapsed the code below since it showed up in a nearly-identical form in a previous post, but you can view it by clicking the “Code” button.)\n\n\nShow code\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\n\n#-------------------\n\n# This function will take two strings of coin flips and return the one that occurs\n# first in a long sequence of coin flips.\n\npenney <- function(p1, p2){         # p1 and p2 are strings like \"HHT\", \"HTT\"\n  \n  coins <- nchar(p1)                # record how many coins are in the sequences to be compared\n    \n  p1 <- unlist(str_split(p1, \"\"))   # split string into vector of length equal to coins\n  p2 <- unlist(str_split(p2, \"\"))\n  \n  flips <- sample(c(\"H\", \"T\"), coins, replace = T) \n    \n  # on each pass of the while loop, push the first n-1 coins forward in the list,\n  # then sample a new coin for the nth one; for instance, HTH becomes TH? where\n  # ? is the result of a new coin flip\n  while(!all(p1 == flips) & !all(p2 == flips)){\n    flips[1:(coins - 1)] <- flips[2:coins]\n    flips[coins] <- sample(c(\"H\", \"T\"), 1)\n  }\n    \n  ifelse(all(p1 == flips), \n         paste0(p1, collapse = \"\"), \n         paste0(p2, collapse = \"\"))  # output winning sequence\n}\n\n#-------------------\n\n# This helper function takes two coin sequences, simulates Penney's game many\n# times, and returns the proportion of times the *first* sequence wins.\npenney_wrapper <- function(p1, p2){\n  mean(replicate(1e4, penney(p1, p2)) == p1)\n}\n\n# The next function produces a vector of appropriate length enumerating all\n# possible sequences of coin flips of that length; for instance, ht_iterator(2)\n# will return the vector c(\"HH\", \"HT\", \"TH\", \"TT\"). This works recursively by \n# calling ht_iterator(k-1), duplicating it, and appending each of H and T to \n# one of the duplicates.\nht_iterator <- function(k){\n  if(k == 1) {\n    return(c(\"H\", \"T\"))\n  } else {\n    ht_iterator(k-1) %>%\n      rep(each = 2) %>%\n      paste0(c(\"H\", \"T\"))\n  } \n}\n\n# make a data frame with all combinations of sequences\nresults <-\n  ht_iterator(3) %>%\n  combn(2) %>%\n  t() %>%\n  as.data.frame()\n\nnames(results) <- c(\"p1\", \"p2\")\n\nresults <- \n  results %>%\n  rowwise() %>%\n  mutate(p1_wins = penney_wrapper(p1, p2)) %>%\n  mutate(p1_wins = round(p1_wins, 2))\n\n# since we simulated only half of the matchups, we can just mirror\n# the results to flip p1 and p2 rather than resimulating\nresults <-\n  tibble(\n    p1 = results$p2,\n    p2 = results$p1,\n    p1_wins = 1 - results$p1_wins\n  ) %>%\n    bind_rows(results)\n\nresults %>%\n  ggplot(aes(x = p1, y = p2, fill = p1_wins)) +\n    geom_tile() +\n    geom_text(aes(label = p1_wins), size = 6) +\n    scale_fill_gradient2(low = \"yellow\", mid = \"white\", \n                         high = \"cyan\", midpoint = 0.5) +\n    labs(title = \"Head-to-head matchups: Penney's Game\",\n         subtitle = \"10K trials per matchup combination\",\n         fill = \"p1 win %\") +\n    theme(text = element_text(size = 20))\n\n\n\n\n\nNext, we explored a non-competitive variant of this game in which a player waits for a sequence to appear. Like before, the result that many find counterintuive at first – and I very much include myself in that group – is that the expected wait time depends on the sequence chosen. (As before, I’ve obscured the code since it appeared in a nearly-identical form in a previous post, but you can click the button below if you want to see it.)\n\n\n\n\nShow code\npenney_wait <- function(p1){        # p1 is a string like \"HHT\"\n  \n  coins <- nchar(p1)                # keep track of how many coins needed\n  p1 <- unlist(str_split(p1, \"\"))   # split string into vector w/ length == coins\n    \n  flips <- sample(c(\"H\", \"T\"), coins, replace = T) \n    \n  num_flips <- coins                # keep track of how many flips have happened\n    \n  # on each pass of the while loop, push the first n-1 coins forward in the list,\n  # then sample a new coin for the nth one; for instance, HTH becomes TH? where\n  # \"?\" is the result of a new coin flip\n  while(!all(p1 == flips)){\n    flips[1:(coins - 1)] <- flips[2:coins]\n    flips[coins] <- sample(c(\"H\", \"T\"), 1)\n    num_flips <- num_flips + 1\n  }\n    \n  num_flips                         # return number of flips required\n}\n\ntibble(sequence = ht_iterator(3)) %>%\n  rowwise() %>%\n  mutate(avg_wait = mean(replicate(1e4, penney_wait(sequence)))) %>%\n  ggplot(aes(x = sequence, y = avg_wait)) +\n    geom_bar(stat = \"identity\") +\n    labs(title = \"Monte Carlo simulations of wait times\",\n         subtitle = \"10,000 total trials per sequence\",\n         y = \"Average wait time\") +\n    theme(text = element_text(size = 20)) +\n    coord_flip()\n\n\n\n\n\nOf course, there are good reasons for these phenomena. As with any interesting mathematical question, you can go through the calculations to verify these facts. And yet, in my experience, most people (mathematicians and non-mathematicians alike) crave an intuitive understanding of the problems that lives on a different plane from the calculations and proof.\nWhen I was first becoming accommodated with Penney’s Game, I reconciled the two graphs above by concluding that coin sequences are naturally tiered. From the graph of average wait times, we see a suggested hierarchy:\n\n\n\nranking\nsequences\n\n\n\n\nslow\nHHH, TTT\n\n\nmedium\nHTH, THT\n\n\nfast\nHHT, HTT, THH, TTH\n\n\n\nIndeed, from examining the graph of head-to-head win probabilities, we see that this is indeed the case; all “fast” sequences carry at least a \\(50\\%\\) win rate over all “medium” and “slow” sequences, and all “medium” sequences also hold this over the “slow” sequences.\nI found this to be quite satisfying, and it led me to wonder: In Penney’s Game, do sequence structures always give rise to this type of hierarchy of both expected values and competitive advantages?\n\n\nThe Incorrect Answer\nIt’s probably clear where I’m heading with this, right? This conjecture seemed completely reasonable to me. I assumed that sequences with lower expected wait times would always have competitive advantages (or at least be fair propositions) against sequences with higher expected wait times.\n\n\nThe Correct Answer\nThere are no hierarchies here, because Penney’s Game is a place where reasonable conjectures go to die. Let’s have a look at the expected wait times for the four-coin version of the game:\n\n# ht_iterator() is defined in the first collapsed code block above\n# penney_wait() is defined in the second collapsed code block above\n\ntibble(sequence = ht_iterator(4)) %>%\n  rowwise() %>%\n  mutate(avg_wait = mean(replicate(1e4, penney_wait(sequence)))) %>%\n  ggplot(aes(x = sequence, y = avg_wait)) +\n    geom_bar(stat = \"identity\") +\n    labs(title = \"Monte Carlo simulations of wait times\",\n         subtitle = \"10,000 total trials per sequence\",\n         y = \"Average wait time\") +\n    theme(text = element_text(size = 20)) +\n    coord_flip() +\n    geom_hline(yintercept = c(16, 18, 20, 30), lty = 2, lwd = 2, color = \"skyblue\")\n\n\n\n\nOne of the challenges of the Monte Carlo approach is that the bars in the graph fluctuate a bit based on the random simulations, and it can be tough to determine what the exact values are visually. However, by a theorem (one that has not appeared in these notes… yet), the true expected wait times of each of these sequences is an even integer.\nThat fact is enough to deduce the true expected wait times despite the fluctuations from simulations. Vertical lines on the graph above have been superimposed at positions \\(x = 16, 18, 20, 30\\). That is:\n\n\n\nsequences\nexpected wait time\ntier\n\n\n\n\nTTTT, HHHH\n30\nslowest\n\n\nHTHT, THTH\n20\nmed. slow\n\n\nTTHT, THTT, THHT, HTTH, HTHH, HHTH\n18\nmed. fast\n\n\nTTTH, TTHH, THHH, HTTT, HHTT, HHHT\n16\nfastest\n\n\n\nIf a hierarchy exists, this is what it would have to be.\nNext, let’s pit all the “fastest” group coins against the “med. fast” group coins competitively. There’s nothing special about this particular choice of tiers to compare, other than that it gives us lots of combinations to pit against each other.\n\nfour_coin_results <- \n  expand.grid(c(\"TTTH\", \"TTHH\", \"THHH\", \"HTTT\", \"HHTT\", \"HHHT\"),\n              c(\"TTHT\", \"THTT\", \"THHT\", \"HTTH\", \"HTHH\", \"HHTH\"),\n             stringsAsFactors = FALSE) %>%\n    as.data.frame() %>%\n    rename(\"p1\" = \"Var1\", \"p2\" = \"Var2\") %>% \n    mutate_all(as.character())\n\nfour_coin_results <-\n  four_coin_results %>%\n    rowwise() %>%\n    mutate(p1_wins = penney_wrapper(p1, p2)) %>%\n    mutate(p1_wins = round(p1_wins, 2))\n\nfour_coin_results %>%\n  ggplot(aes(x = p1, y = p2, fill = p1_wins)) +\n    geom_tile() +\n    geom_text(aes(label = p1_wins), size = 6) +\n    scale_fill_gradient2(low = \"yellow\", mid = \"white\", \n                         high = \"cyan\", midpoint = 0.5) +\n    labs(title = \"Head-to-head matchups: Penney's Game\",\n         subtitle = \"10K trials per matchup combination\",\n         fill = \"p1 win %\",\n         x = \"p1 (fastest group)\",\n         y = \"p2 (med. fast group)\") +\n    theme(text = element_text(size = 20))\n\n\n\n\nA few coins in the “fastest” group do indeed dominate those in the “med. fast” group – but not all of them. HHTT, HTTT, THHH, and TTTH all have a competitive disadvantage against at least one coin from the allegedly lower tier.\nGoodbye, Reasonable Conjecture… we hardly knew ye.\n\n\nThe Big Lesson\nThe most obvious lesson is that Penney’s Game is weird – which, sure. That’s at least the third time we’ve learned that particular lesson. More generally, we might use this as a reminder that probability is home to lots of counterintuitive results.\nBut I think the most important part of this story comes back to the idea of a mathematical laboratory. The Penney’s Game saga so far naturally invites several questions for further exploration, such as:\n\nIf I know the sequence chosen by my opponent, how do I build the sequence with the best advantage over it?\nCan a shorter sequence (like TTT) ever have a faster wait time than a longer sequence (like HTHH)?\n\nWe’ve actually seen that the answer is no for sequences of length 3 and 4, but what about longer sequences?\n\nIs there any kind of pattern in the expected wait times as the sequence lengths increase?\nCan a longer sequence ever have a direct head-to-head competitive advantage over a shorter sequence?\n\nFrancis Su writes in his wonderful book Mathematics for Human Flourishing about what it means to be a mathematical explorer:\n\nExploration cultivates an expectation of enchantment. Explorers are excited by the thrill of finding the unexpected, especially things weird and wonderful. It’s why hikes through unfamiliar terrain entice us, why unexplored caves beckon to us, why the strange creatures of the deep-sea ocean floor fascinate us – what else may be lurking down there? There’s similar enchantment to be found in the zoo of strange discoveries in math.\n\nAnyone with any level of expertise in probability (including none), and the ability to write code, can engage in investigations like those above. Thanks to Monte Carlo simulations, the joy of exploration in Penney’s Game can suddenly become accessible as a new class of explorers are invited to broaden the scope of who can partake in the beauty of mathematics. This is the way that Monte Carlo simulations can function as a mathematical laboratory; they can be used as a vessel for exploration and discovery.\nIf I could change one thing about my own experience working toward my PhD in Mathematics, I would have embraced Monte Carlo simulations much earlier in my career. I was able to construct them as a graduate student, but since I had never considered them in a particularly structured way, I tended not to reach for that tool when working on problems. In hindsight, I think engaging more in simulations early in my career could have sharpened my intuition, confirmed difficult theoretical calculations, and maybe even suggested new theorems to try to prove.\n\n\nTo Be Continued…\nAnd yet, Monte Carlo simulations are also limited in scope. The ability of simulations to suggest conjectures is profound; their ability to prove those conjectures is flimsy at best. They typically fall short of showing why things are the way they are.\nIn Penney’s Game, some of the “why” questions might include:\n\nWhy do some sequences have competitive advantages against each other?\nWhat are the structural considerations of sequences that inform their expected wait times or competitive advantages?\nWhat are the exact win probabilities between sequences that are reflected as mere shadows in Monte Carlo simulations above?\nWhy can different sequences have different expected wait times?\nWhy are the expected wait times of any sequence always even integers?\n\nQuestions like these require good old-fashioned mathematical arguments. Those are up next."
  },
  {
    "objectID": "posts/2023-01-06-dsa-101-diary-1/index.html",
    "href": "posts/2023-01-06-dsa-101-diary-1/index.html",
    "title": "Teaching Data 101 with Dataquest",
    "section": "",
    "text": "dataquest.io \n\n\n\n\nOverview\nI’m taking a quick break from the usual “weird tales from probability, statistics, and data” oeuvre of this blog. For this post, I’d like to write about something else that’s near and dear to my both my heart and my daily schedule: teaching an Introduction to Data Science and Analytics course.\nThis course (DSA-101 for short) is the foundational, introductory-level course for the new Data Science and Data Analytics programs we have launched this year at Baldwin Wallace University. In lieu of a textbook, I decided to adopt Dataquest for use in our class.\nHere’s where I should put a disclosure – or, rather, a lack thereof. I am not affiliated with Dataquest; I am not paid by Dataquest in any capacity, particularly including for using their platform in my class. Nobody at Dataquest has solicited this post in any fashion from me. I’m writing about this because, simply put, Dataquest is a well-made product and I believe in it.\nMore practically, I’m writing this post with some actionable tips for anyone who would like to try using Dataquest in a structured classroom environment. This entire class was an experiment for me; some things worked well, some were total flops, and some were in between. I’m hoping that by documenting my experience here, I might be able to provide a roadmap for anyone else who would like to try this… including myself, when I teach another section of this class in the upcoming Spring semester.\n\n\nWhat is Dataquest?\nDataquest is a collection of online courses covering data topics. Many similar products exist, but what makes Dataquest special is its blend of structured lessons and open-ended projects. Moreover, it has clearly been assembled by instructors who understand education and the crucial role of nontrivial active learning. It is challenging, but also supportive; it asks much of learners, but it provides enough scaffolding for them to get there.\nI am a huge fan of Dataquest, and these defining features are why:\n\nThe program doesn’t take educational shortcuts. Learners write code on a blank prompt instead of filling in blanks or modifying existing code. Exercises ask students to think, not to trivially adjust things they’ve already seen. Descriptions are given in text and graphics, not in videos that make the learner feel good while subconsciously zoning out. Completing the lessons doesn’t feel like following instructions; it feels more like solving problems.\nProjects invite meaningful creativity. The projects interspersed throughout the curriculum are open-ended and challenging. These projects embrace the idea that there are multiple correct ways to solve problems, and that different people may arrive at different conclusions. A student from my previous semester described one of these projects as “hard, but in a good way.”\nIt’s fun. When I was preparing the DSA-101 course, I decided to complete the R tidyverse modules on my own first so that I deeply understood what I was asking my students to do. This was much more fun than I expected, and I learned a lot while completing it. Even though I’ll still be using the R content for the DSA-101 class this semester, I’ve decided to learn some Python on the side, just because I can. I think most learning is intrinsically fun, and Dataquest does a wonderful job of enabling the fun to naturally occur.\n\nThe Dataquest Academic Program provides a way for instructors to use the Dataquest platform in their classes at no charge. I kept waiting all semester to see what the catch would be, and… it just never came. We were generously granted Dataquest use for the entire semester, for free. Given that I was using it like a textbook, and functionally in place of a textbook, my students were pretty excited about this.\n\n\n\nDataquest Screenshot\n\n\n\n\nUsing Dataquest in DSA-101\nSince our DSA-101 course serves as the cornerstone of our new data majors, I had some design considerations as I constructed the course. Dataquest includes robust coverage of both the R and Python programming languages; I opted to use R because my university’s data programs already required multiple other Python-based courses from our Computer Science department, but did not already have any coverage of R’s tidyverse.\nStudents entered the course with a wide variety of backgrounds; some students were senior-level Computer Science majors looking to add some data skills, and others had no programming experience at all. Since most of the veteran coders’ experience was in Python or Java, using R leveled the playing field somewhat and made it easier for all students to relate to one another as learners.\nThe typical class experience was similar to a “flipped” class. During our class time, students worked on their own to complete Dataquest modules that extended to homework on an as-needed basis. Students also spent significant time working in groups on formative group quizzes meant to foster discussion.\n\n\nWhat worked well\nHere are the things about the course that I would do again and would recommend for any other instructors who are thinking about using the Dataquest Academic Program:\n\nA customized content path: None of the existing career paths in Dataquest quite fit what I was hoping to do in the class, so I carefully curated a list of the modules that I asked my students to complete.\nStaying focused on deliverables: As I chose the modules used in my class, I kept my attention on what I wanted them to be able to make. My first goal was for them to use ggplot2 to construct some very basic graphs; we were able to achieve this goal in just over four weeks. Once we got there, data visualization suddenly became a tool that we could use to discuss broader data concepts. Being able to create things helped give the course a feel of relevance.\nBalancing autonomy against a common pace: The veteran coders still had an obvious advantage in speed when picking up the R content. Allowing students to work on their own during class, and having homework naturally arise as spillover content for what they were not able to complete, mostly worked quite well to balance the different learning speeds.\n\n\n\n\nDataquest Screenshot\n\n\n\n\nWhat could have gone better\nHere are the biggest areas where my use of Dataquest fell short of what I want to do as an instructor:\n\nFostering conversation and community: When students were working on Dataquest content, the room was too quiet! I encouraged them to talk to each other and work together, but mostly they wanted to work independently. We eventually settled on playing music during Dataquest sessions, which helped – but next semester I’ll also place a greater emphasis on group work outside Dataquest to get them talking and sharing ideas.\nGaps in course content relative to my goals: There were a few topics that I was hoping to cover in the course for which I didn’t find suitable Dataquest coverage. In some cases, the content didn’t exist; in others, it was too long for me to use in the 15-week course. As such, I had to develop some content on my own outside the Dataquest platform, which didn’t go as well as I would have liked.\nBuilt-in Dataquest projects as assessment: The Dataquest projects are very, very good; however, I did not find them to work well for the graded assessments. The biggest issue was that the availability of the solution notebooks implicitly invited students to examine solutions and deprive themselves of chances to think. This isn’t a problem with the Dataquest platform, but rather with the concept of grading, which naturally twists incentives from learning to getting high scores. Next semester, I’ll write new original projects to nudge students away from this behavior.\n\n\n\nOther tips and tricks\nSome pieces of advice I would have for any instructors using the Dataquest Academic Program:\n\nBefore you ask a class to complete something, go through it yourself first. This seems like an obvious teaching tip, but it’s worth reiterating here; I was tempted to skip some modules I wanted to assign to my students on basic R things that I knew well. However, completing the modules on my own first allowed me to deeply understand what my students would have seen and to predict places where they would struggle. This also put into clearer focus which modules were worth our limited class time.\nSupplement Dataquest content with structured questions. After most lessons, I gave my class something called “zero-stakes quizzes” that were designed to give them something to work on together outside the Dataquest content. This allowed me to both reinforce the skills conveyed in Dataquest and to push them a bit further on things that weren’t covered by the modules; it also got them talking to each other, which helped improve the feel of the class.\n\n\n\nConclusion\nI really can’t say enough good things about Dataquest and my experience using it in my class. Whether you’re someone looking to learn some data and programming concepts, or you’re an instructor looking to provide some structured assistance for students, Dataquest is worth checking out. (There’s a free trial available if you want to give it a shot!)"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Coming Soon….",
    "section": "",
    "text": "Please excuse my dust! I’m in the middle of rebuilding my old blog. You win three internet points for finding this temporary page!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "unindexed/uniform_numbers_solution.html",
    "href": "unindexed/uniform_numbers_solution.html",
    "title": "Uniform Products: Solution",
    "section": "",
    "text": "When we compute a product of many numbers that are each uniformly distributed on \\([0, C]\\), the product becomes very small when \\(C = 1\\) and becomes very large when \\(C = 10\\). Where is the “cutoff” \\(C\\) where the product exhibits a transition?"
  },
  {
    "objectID": "unindexed/uniform_numbers_solution.html#proof",
    "href": "unindexed/uniform_numbers_solution.html#proof",
    "title": "Uniform Products: Solution",
    "section": "Proof",
    "text": "Proof\nLet’s formulate this mathematically: we want to know how the product \\[U_n = \\prod_{k=1}^{n} X_k\\] behaves, where \\(\\{X_i\\}\\) is a collection of independent, identically-distributed numbers that are all uniformly distributed on \\([0, C]\\). Products of random variables are somewhat obnoxious to work with; luckily, logarithms1 will let us trade products for sums. \\[\\log(U_n) = \\log \\left( \\prod_{k=1}^{n} X_k \\right) =\n\\sum_{k=1}^{n} \\log \\left( X_k \\right)\\]\nOne advantage of a sum is that it more closely resembles an average:\n\\[\\frac{\\log(U_n)}{n} = \\frac 1 n \\sum_{k=1}^n \\log \\left( X_k \\right)\\]\nNow, the right-hand side is nothing more than an average of \\(n\\) independent copies of \\(\\log(X_i)\\). The Law of Large Numbers says that as \\(n\\) increases, the right-hand side will converge to the expected value \\(\\mathbb E[\\log(X_i)]\\), so we just need to calculate that. By the Law of the Unconscious Statistician, this is \\[\\int_0^C \\frac 1 C \\cdot \\log(x) \\, \\textrm d x = \\frac 1 C \\left[x \\log(x) - x \\right] \\bigg|_0^C = \\log(C) - 1.\\]\nPutting it all together, we can now investigate the behavior of the original product \\(U_n\\). As \\(n\\) increases, \\(\\frac{\\log(U_n)}{n}\\) converges to \\(\\log(C) - 1\\).\n\nWhen \\(C > e\\), \\(\\frac{\\log(U_n)}{n}\\) converges to a positive constant; hence, \\(\\log(U_n)\\) diverges to \\(\\infty\\), so \\(U_n\\) does so as well.\nWhen \\(C < e\\), \\(\\frac{\\log(U_n)}{n}\\) converges to a negative constant; in this case, \\(\\log(U_n)\\) diverges to \\(-\\infty\\), implying that \\(U_n\\) converges to \\(0\\).\n\nHence, the “transition point” for \\(C\\) is indeed \\(e\\)."
  },
  {
    "objectID": "unindexed/uniform_numbers_solution.html#but-wait",
    "href": "unindexed/uniform_numbers_solution.html#but-wait",
    "title": "Uniform Product Transition: Solution",
    "section": "But wait!",
    "text": "But wait!\nWe left out one case! What happens when \\(C = e\\)?\nFILL IN"
  },
  {
    "objectID": "unindexed/test_post.html",
    "href": "unindexed/test_post.html",
    "title": "test post",
    "section": "",
    "text": "this is a test! let’s see what it does…"
  },
  {
    "objectID": "posts/2023-01-30-chatbots-education/index.html",
    "href": "posts/2023-01-30-chatbots-education/index.html",
    "title": "ChatGPT: A Calculator for the Humanities?",
    "section": "",
    "text": "Photo by Andrea De Santis on Unsplash \n\n\n\nIf you throw a rock at the internet right now, you’re likely to hit an article venting concerns about the role that ChatGPT may play in helping students cheat. Essentially overnight, asking students to write a short essay has become somewhat automatable. The world now has a machine that can generate original prose, and for now, it will do so for free. Usually, its writing style is decent or better. Sometimes, it gets facts correct. That is: it can passably accomplish the tasks educators often ask of those learning to write, and it will do so on demand in a matter of seconds.\nFor many, especially this sudden shift in the status quo is terrifying. I absolutely understand and empathize with this reaction. But, I also think we can chart a course forward by taking a lesson from math education.\n\nSolving Equations\nI’ll ask you to grant me a premise: let’s suppose that you’re a student in a calculus class today – that is, in the year 2023. A teacher has asked you to solve the equation \\(x^5 - x + 1 = 0\\) by providing an answer that is correct to three places beyond the decimal. This is a pretty silly premise, and it’s an especially silly equation, but let’s run with it.\nIn the early twentieth century, there were many numerical tactics for solving equations like that. One of my personal favorites is Newton’s Method, which is brilliantly illustrated in this graphic (courtesy of Wikipedia):\n\n\n\nNewton’s Method\n\n\nNewton’s method is an absolutely delightful and clever use of basic calculus concepts wrapped together to give good approximations of solutions to equations. I mean it unironically when I say that this strategy is beautiful. This would be a great tool for approximating solutions to our equation at hand. Indeed, even fifty years ago, that might have been exactly the kind of tool a student would have used to answer the original question.\nIt is also not what a student would use today.\nA modern high school student would (hopefully) use a graphing calculator to answer this question. By graphing the nonzero side of the equation, they’d probably see something like this:\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# define the polynomial and the x-coordinates to be used in the graph\nexample_polynomial = np.poly1d([1,0,0,0,-1,1])\nx_coords_graph = np.linspace(-1.5, 1, 50)\n\n# find the real roots of the polynomial\nexample_solution = [i for i in np.roots(example_polynomial) if np.imag(i) == 0]\n\n# extract the real component, round it, and remove it from the ndarray\nexample_solution = np.real(example_solution).round(3)[0]\n\nfig, ax = plt.subplots()\n\nax.plot(x_coords_graph, example_polynomial(x_coords_graph))\n\n# axis lines\nax.axhline(color = 'black', linestyle = ':')\nax.axvline(color = 'black', linestyle = ':')\n\n# add square on location of root\nax.add_patch(plt.Rectangle((example_solution - 0.02, -0.1), width = 0.04, height = 0.2, color = 'black'))\n\nax.text(x = -1.3, y = -3, s = '$f(x) = x^5 - x + 1$', size = 12)\nax.text(x = example_solution, y = -0.5, s = '$({} , 0)$'.format(example_solution))\n\nplt.show()\n\n\n\n\n\nSince we just want a root of the polynomial, many modern tools would just write it directly on the graph, like this. If the tool didn’t, students would still have options for proceeding; they could zoom in near the black intersection point to discover its \\(x\\)-coordinate to the necessary precision, or they may be able to use an equation solver built into their calculator to get what they need.\nNewton’s Method – the beautiful strategy I mentioned earlier – isn’t what students would reach for anymore, especially as a first option.\nWhen calculators first entered the classroom, many math educators worried about what would be lost if we de-emphasized computation. And to be clear, some things have been lost; for instance, Newton’s Method now enjoys a less prominent role in modern calculus classes than it used to. Students also no longer routinely graph functions by hand, even though such an act does require careful thought and synthesis of many mathematical concepts. Many other mathematical techniques have slowly faded into the background due to the techno-cultural revolution ushered in by graphing calculators.\nAnd yet, by any measurable standard, things in mathematics education are better now than before the arrival of graphing calculators. More students are learning more things at a deeper level, even if those things aren’t the same collection of things as they used to be.\nIn recognition that tasks like approximating solutions to equations are less interesting than they used to be, mathematics education has changed. Priorities have shifted, and teaching strategies and assessments have followed. The discipline has evolved.\n\n\nChatbots and Writing\nI can’t help but wonder if the release of ChatGPT, and the deluge of similar AI tools that are sure to follow in the coming months, could be a calculator moment for humanities. Entire curricula have been developed around asking students to demonstrate understanding in the form of written prose. Tools like TurnItIn have become popular as ways to identify if students are submitting work written by others. These tools were built on the premise that unoriginal work would probably be duplicated elsewhere. This was a nice premise; may it forever rest in peace. Entire apparatuses have been unpheaved. It’s not that change is coming; change is now here.\nLet’s lean into it.\nFirst, we should get the ugly part out of the way. If educators give the same assessments that we’ve always given, then yes, students will use ChatGPT to take shortcuts and submit work that isn’t theirs. This is a great reason for educators to change up their assessments.\nSecond: although many worry that students may submit work generated by a chatbot, let’s remember what the chatbot does: it is a probabilistic model that predicts the likeliest thing to come next in a sequence. It is, reductively, autocomplete on steroids. On its best day, it delivers what is almost by definition an average thing that one might find on the internet. On its worst day, it gets facts completely wrong (but it will do so in very nice prose).\nCurrently, average things are generally regarded to be acceptable. But what if chatbot output became the new floor, rather than a middling thing begrudgingly regarded as sufficient? Is there a room in education to design around this new technology by asking students to start from it and build atop it?\n\n\n\nBloom’s Taxonomy\n\n\nFor the most part, chatbots are bottomfeeders in Bloom’s taxonomy. They can recall most things with decent accuracy, but they understand nothing. They will eloquently explain their lack of understanding to you if you ask them, and they will demonstrate it to you if you give them the chance. They cannot reason; they can provide you with “I Can’t Believe It’s Not Reason,” i.e. the output of statistical models that typically falls apart admist modest provocation.\nThere might be an opportunity here for educators to reset expectations. Once, math classes asked students to manually construct graphs of functions; now, math classes ask students to use graphs of functions to solve problems. Perhaps, instead of hoping only for students to use words well – which a machine can now do with reasonable fidelity – perhaps we can ask for those words to be interesting.\n\n\nActionable Suggestions for Educators\nHere’s what I’ve decided to do as an educator regarding the advent of chatbots. I won’t be so arrogant as to call this “advice,” because I have no idea if it will work, but it feels right to me in this cultural moment.\n\nPlay with it. This is the most important suggestion imaginable. Every instructor must internalize what this tool can do, and just as importantly, what it can’t do. Feed it your own homework assignments. Toss it questions to which you already know the answers. Toss it questions to which you don’t know the answers, then look up the answers to see how it did. Don’t rely on articles riding the hype wave to understand this tool; if you’re an educator, you know the potency of experiential learning, and the same principle applies here. The limitations of chatbots are just as important as their capabilities, and you will only deeply understand either by trying it.\nConsider whether the tool has a place for learning in your classroom. At the top of this post, I had a graph of the polynomial from the opening question; for any interested readers, there’s a “Show Code” button right above the graph. I’ve been learning Python for the last few months, and I wanted to use that graph as a small exercise for myself to reinforce my own learning. I hadn’t done anything with polynomials yet, so I needed some new syntax; I asked ChatGPT for some help (“How do I graph polynomials in Python?”), and it pointed me to np.poly1d() with a helpful example in code. I should mention that I didn’t actually use any of the code samples it gave me; rather, I broke off its suggestion of np.poly1d() and incorporated it into my own code that I cobbled together through the usual means (i.e. documentation, previous code I had written, and Stack Overflow questions). Essentially, I used it in place of a Google search, and I found it to be very helpful. If I found it suitable for such use myself, I feel that I owe it to my students to share that use with them.\nIf something is now lost, ask yourself what remains important. You will probably find that some things from your classes – perhaps many of them – are now automatable. Ask yourself: if a task can be trivially automated, is it worth continuing to assess students’ mastery over it? The answer may well be yes! As Philippe Barbe recently observed, we still teach multiplication tables in grade school, and for very good reason, despite the existence of calculators. It is absolutely fine to decide that a skill remains a relevant part of the learning process despite its automation. But educators have a moral obligation to be continually intentional about such decisions.\nOnce you have decided on the important learning outcomes, put students in a room and have them show that they meet the standards. Perhaps that means restricting their technology access so that they do not have AI tools; or, perhaps it means using AI tools as an integrated part of assessments.\n\nI would suggest a potential North Star for course design, a uniform learning outcome that cuts across all disciplines: good classrooms should foster the ability to think. Everything else is secondary, and anything that was previously just a proxy for thinking can be discarded or replaced."
  },
  {
    "objectID": "posts/2023-04-10-uniform-products/index.html",
    "href": "posts/2023-04-10-uniform-products/index.html",
    "title": "Uniform Products",
    "section": "",
    "text": "Photo by Mika Baumeister on Unsplash \n\n\n\n\nUniformly-distributed numbers\nLet’s start with a refresher of uniform numbers: numbers that are uniformly- distributed on an interval \\([a, b]\\) can assume any decimal value in that range without preference to any particular subinterval. They’re the usual thing people mean when they talk about “random numbers”. Here’s a bit of Python code to generate five uniformly-distributed numbers on \\([0, 1]\\):1\n\nfrom numpy.random import default_rng\nrng = default_rng()\n\nprint(rng.uniform(low=0, high=1, size=5))\n\n[0.39763174 0.65915304 0.58568506 0.60475324 0.94004794]\n\n\nNote that these numbers are independent And here’s a visualization of ten thousand numbers generated on that same interval.\nINSERT VISUALIZATION HERE\nnarrative about product\n\nrng.uniform(low=0, high=10, size=10000).prod()\n\ninf\n\n\nGoogle Colab notebook\n\n\n\n\n\nFootnotes\n\n\nTechnically, these are pseudorandom numbers; von Neumann might accuse me of living in a state of sin if I conflate the two.↩︎"
  },
  {
    "objectID": "posts/2023-04-12-uniform-products/index.html",
    "href": "posts/2023-04-12-uniform-products/index.html",
    "title": "Uniform Products: Problem",
    "section": "",
    "text": "Photo by Mika Baumeister on Unsplash \n\n\n\n\nUniformly-distributed numbers\nLet’s start with a refresher of uniform numbers: numbers that are uniformly- distributed on an interval \\([a, b]\\) can assume any decimal value in that range without preference to any particular subinterval. They’re the usual thing people mean when they talk about “random numbers”. Here’s a bit of Python code to generate five independent uniformly-distributed numbers on \\([0, 1]\\):1\n\nimport numpy as np\n# establish random number generator with seed for reproducibility\nrng = np.random.default_rng(seed=6538)\n\nprint(rng.uniform(low=0, high=1, size=5))\n\n[0.79730922 0.18164333 0.11063476 0.81771383 0.18290457]\n\n\nAnd here’s a visualization of ten thousand uniformly-distributed numbers generated on \\([0, 1]\\):\n\n\nShow code\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.hist(rng.uniform(low=0, high=1, size=10000),\n        bins=10,\n        edgecolor='black'\n        )\nax.set_title('10K random numbers uniformly distributed on [0,1]')\nplt.show()\n\n\n\n\n\nAll bars have roughly equal heights with no discernible trends. Now, let’s ask an interesting question: What happens if we multiply all these numbers together?\n\nprint(rng.uniform(low=0, high=1, size=10000).prod())\n\n0.0\n\n\nOkay, I lied; that’s not interesting at all. Since all those numbers are less than 1, every time we throw a new one into the product, the product will just get smaller. The end result will be something positive, but computationally indistinguishable from 0. Let’s watch this in action by graphing how the products evolve after just a few terms:\n\n\nShow code\n# length of sample path, i.e. number of uniformly-distributed numbers within\n# each product group\nn = 15\n\n# generate ten sample paths of random walk, organized in columns\nuniform_nums = rng.uniform(low=0, high=1, size=10*n).reshape([n, 10])\n\n# find the cumulative products within columns, starting from the top;\n# the kth element in a given column is the product of the k numbers that lie on \n# or above it\nsample_path = uniform_nums.cumprod(axis = 0)\n\nfig, ax = plt.subplots()\n\n# plot each of the columns as separate paths\nfor i in range(10):\n    ax.plot(np.linspace(1,n,n), sample_path[:, i], alpha=0.5)\n\nax.set_title('Cumulative Products: Random Numbers on [0, 1]')\nax.set_xlabel('Number of terms in product')\nax.set_ylabel('Product')\n\nplt.show()\n\n\n\n\n\nThis is a visualization of ten different ‘runs’ of products, each of which consists of multiplying fifteen numbers uniformly drawn on \\([0, 1]\\). As expected, the products collapse to 0 almost immediately.\n\nSo, what if we make the numbers bigger? Instead of restricting them to \\([0, 1]\\), let’s spread them out a bit and consider numbers uniformly distributed on \\([0, 10]\\). This time, about \\(10\\%\\) of the numbers will be less than 1 (and thereby make the product smaller), but the other \\(90\\%\\) of the numbers will be between 1 and 10 (and thereby make the product bigger). Maybe that will be enough….?\n\nprint(rng.uniform(low=0, high=10, size=10000).prod())\n\ninf\n\n\nYep! This time, we get a huge number that Python can’t distinguish from infinity. Let’s visualize this like before:\n\n\nShow code\n# length of sample path, i.e. number of uniformly-distributed numbers within\n# each product group\nn = 15\n\n# generate ten sample paths of random walk, organized in columns\nuniform_nums = rng.uniform(low=0, high=10, size=10*n).reshape([n, 10])\n\n# find the cumulative products within columns, starting from the top;\n# the kth element in a given column is the product of the k numbers that lie on \n# or above it\nsample_path = uniform_nums.cumprod(axis = 0)\n\nfig, ax = plt.subplots()\n\n# plot each of the columns as separate paths\nfor i in range(10):\n    ax.plot(np.linspace(1,n,n), sample_path[:, i], alpha=0.5)\n\nax.set_title('Cumulative Products: Random Numbers on [0, 10]')\nax.set_xlabel('Number of terms in product')\nax.set_ylabel('Product')\n\nplt.show()\n\n\n\n\n\nLooking carefully at the y-axis shows that after just fifteen terms, a few of our products became very large; the largest of the ten trials in the graph above is about 6 billion, and five of the ten trials are over 1 billion. Putting all this together, we might (correctly) guess what happens: the more terms we throw into the product, the higher it goes, generally speaking.\n\nSo, to recap: we’ve computed a product of many numbers generated uniformly on \\([0, C]\\) with two different values of \\(C\\).\n\nWhen \\(C = 1\\), the product collapsed down to \\(0\\).\nWhen \\(C = 10\\), the product exploded toward infinity.\n\nNow, we actually have an interesting question… Where is the cutoff? That is, for what value of \\(C\\) does the product switch from being very small to being very large?\n\n\nThe Incorrect Answer\nOne reasonable first guess is that something interesting might happen when \\(C = 2\\). If our numbers come from \\([0, 2]\\), then half of them will make the cumulative product bigger, and half of them would make it smaller.\nBut….\n\nrng.uniform(low=0, high=2, size=10000).prod()\n\n0.0\n\n\nAnd just to make sure that \\(2\\) is not right on the boundary of some change in behavior….\n\nrng.uniform(low=0, high=2.1, size=10000).prod()\n\n0.0\n\n\nSo, the change of behavior doesn’t come at \\(C = 2\\); evidently, it’s something higher than \\(2\\) (and lower than \\(10\\)).\n\n\nThe Correct Answer\nBefore I reveal the correct answer, I want to point out that you, the reader, can play around with this yourself! If you have a place to run some Python code, here’s a snippet to get started:\n\nfrom numpy.random import default_rng\nrng = default_rng()\nprint(rng.uniform(low=0, high=10, size=10000).prod())\n\nIf you don’t have a place to run Python code, I can provide one! Here’s a Google Colab notebook that you can open and run right away; no setup or Python installation required on your local machine at all. You can use this Colab notebook to experiment with the problem even if you have no prior experience in programming.\nWhen you’re ready to see the answer, it’s here.\n\n\n\n\n\nFootnotes\n\n\nTechnically, these are pseudorandom numbers; von Neumann might accuse me of living in a state of sin if I conflate the two. They’re also just floating-point approximations of real numbers, not the genuine article. That’s all okay; the magic will happen anyway despite these indiscretions.↩︎"
  },
  {
    "objectID": "unindexed/uniform_numbers_solution.html#but-wait-theres-more",
    "href": "unindexed/uniform_numbers_solution.html#but-wait-theres-more",
    "title": "Uniform Products: Solution",
    "section": "But wait, there’s more!",
    "text": "But wait, there’s more!\nWe left something out! What happens when \\(C = e\\)?\nLet’s back up a bit: since \\(\\log(U_n) = \\sum \\log(X_k)\\), and we have specifically chosen a value of \\(C\\) to force \\(\\mathbb E[\\log(X_k)] = 0\\), we can see that \\(\\log(U_n)\\) is an unbiased random walk (i.e. a sum whose increments all have mean 0). Moreover, these increments are pretty well-behaved. For instance, they have finite second moments:2\n\\[ \\mathbb E \\left[ \\left(\\log(X_k) \\right)^2 \\right] = \\int_0^e \\frac 1 e \\cdot \\log(x)^2 \\, \\textrm d x = \\frac 1 e \\left[ x(\\log^2(x) - 2 \\log(x) + 2) \\right] \\bigg|_0^e = 1\\]\nHence, the increments \\(\\log(X_k)\\) have mean \\(0\\) and variance \\(1\\). This practically screams for the involvement of everybody’s favorite statistical workhorse: the Central Limit Theorem. The CLT shows us that tells us that if we had scaled \\(\\log(U_n)\\) by \\(\\sqrt n\\) instead of by \\(n\\), then \\(\\frac{\\log(U_n)}{\\sqrt n}\\) would do something interesting – specifically, it would converge (in distribution) to the standard normal distribution. The Hewitt-Savage 0-1 Law implies that there are some values \\(\\phi_{\\inf}, \\phi_{\\sup} \\in [-\\infty, \\infty]\\) such that \\(\\limsup_n \\log(U_n) = \\phi_{\\inf}\\) and \\(\\liminf_n \\log(U_n) = \\phi_{\\sup}\\) almost surely; hence, \\(\\phi_{\\inf} = -\\infty\\) and \\(\\phi_{\\sup} = \\infty\\) (since anything else would force \\(\\log(U_n) / \\sqrt n\\) to collapse to 0, which it doesn’t). This means that \\(\\log(U_n)\\) is a random walk that will get arbitrarily high and arbitrarily low, infinitely often, almost surely.\nOr, to summarize the end of that story in plain English: when \\(C = e\\), the product \\(U_n\\) will be a random walk in the positive real number line; it will wander very high up, and will wander very close to \\(0\\), and will do both of those things many times.\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.random import default_rng\nrng = default_rng(seed=6538)\n\nn = int(1e6)\n\n# generate the random numbers\nuniform_nums = rng.uniform(low=0, high= 2.718281828459045, size=n)\n\n# find the cumulative products:\n# the kth element of the array is the product of the first k numbers\nsample_path = np.cumprod(uniform_nums)\n\nfig, ax = plt.subplots()\nax.plot(np.linspace(1,n,n), sample_path)\nax.set_yscale('log')\nplt.show()"
  },
  {
    "objectID": "posts/2023-04-11-uniform-products-solution/index.html",
    "href": "posts/2023-04-11-uniform-products-solution/index.html",
    "title": "Uniform Products: Solution",
    "section": "",
    "text": "Code to generate this graph can be found above Figure 1\nThis is the solution for the problem originally posed here."
  },
  {
    "objectID": "posts/2023-04-11-uniform-products-solution/index.html#proof",
    "href": "posts/2023-04-11-uniform-products-solution/index.html#proof",
    "title": "Uniform Products: Solution",
    "section": "Proof",
    "text": "Proof\nLet’s formulate this mathematically: we want to know how the product \\[U_n = \\prod_{k=1}^{n} X_k\\] behaves, where \\(\\{X_i\\}\\) is a collection of independent, identically-distributed numbers that are all uniformly distributed on \\([0, C]\\). Products of random variables are somewhat obnoxious to work with; luckily, logarithms1 will let us trade products for sums. \\[\\log(U_n) = \\log \\left( \\prod_{k=1}^{n} X_k \\right) =\n\\sum_{k=1}^{n} \\log \\left( X_k \\right)\\]\nOne advantage of a sum is that it more closely resembles an average:\n\\[\\frac{\\log(U_n)}{n} = \\frac 1 n \\sum_{k=1}^n \\log \\left( X_k \\right)\\]\nNow, the right-hand side is nothing more than an average of \\(n\\) independent copies of \\(\\log(X_i)\\). The Law of Large Numbers says that as \\(n\\) increases, the right-hand side will converge to the expected value \\(\\mathbb E[\\log(X_i)]\\), so we just need to calculate that. By the Law of the Unconscious Statistician, this is \\[\\int_0^C \\frac 1 C \\cdot \\log(x) \\, \\textrm d x = \\frac 1 C \\left[x \\log(x) - x \\right] \\bigg|_0^C = \\log(C) - 1.\\]\nPutting it all together, we can now investigate the behavior of the original product \\(U_n\\). As \\(n\\) increases, \\(\\frac{\\log(U_n)}{n}\\) converges to \\(\\log(C) - 1\\).\n\nWhen \\(C > e\\), \\(\\frac{\\log(U_n)}{n}\\) converges to a positive constant; hence, \\(\\log(U_n)\\) diverges to \\(\\infty\\), so \\(U_n\\) does so as well.\nWhen \\(C < e\\), \\(\\frac{\\log(U_n)}{n}\\) converges to a negative constant; in this case, \\(\\log(U_n)\\) diverges to \\(-\\infty\\), implying that \\(U_n\\) converges to \\(0\\).\n\nHence, the “transition point” for \\(C\\) is indeed \\(e\\)."
  },
  {
    "objectID": "posts/2023-04-11-uniform-products-solution/index.html#but-wait-theres-more",
    "href": "posts/2023-04-11-uniform-products-solution/index.html#but-wait-theres-more",
    "title": "Uniform Products: Solution",
    "section": "But wait, there’s more!",
    "text": "But wait, there’s more!\nWe left something out! What happens when \\(C = e\\)?\nLet’s back up a bit: since \\(\\log(U_n) = \\sum \\log(X_k)\\), and we have specifically chosen a value of \\(C\\) to force \\(\\mathbb E[\\log(X_k)] = 0\\), we can see that \\(\\log(U_n)\\) is an unbiased random walk (i.e. a sum whose increments all have mean 0). Moreover, these increments are pretty well-behaved. For instance, they have finite second moments:2\n\\[ \\mathbb E \\left[ \\left(\\log(X_k) \\right)^2 \\right] = \\int_0^e \\frac 1 e \\cdot \\log(x)^2 \\, \\textrm d x = \\frac 1 e \\left[ x(\\log^2(x) - 2 \\log(x) + 2) \\right] \\bigg|_0^e = 1\\]\nHence, the increments \\(\\log(X_k)\\) have mean \\(0\\) and variance \\(1\\). This practically screams for the involvement of everybody’s favorite statistical workhorse: the Central Limit Theorem. The CLT shows us that tells us that if we had scaled \\(\\log(U_n)\\) by \\(\\sqrt n\\) instead of by \\(n\\), then \\(\\frac{\\log(U_n)}{\\sqrt n}\\) would do something interesting – specifically, it would converge (in distribution) to the standard normal distribution. The Hewitt-Savage 0-1 Law implies that there are some values \\(\\phi_{\\inf}, \\phi_{\\sup} \\in [-\\infty, \\infty]\\) such that \\(\\limsup_n \\log(U_n) = \\phi_{\\inf}\\) and \\(\\liminf_n \\log(U_n) = \\phi_{\\sup}\\) almost surely; hence, \\(\\phi_{\\inf} = -\\infty\\) and \\(\\phi_{\\sup} = \\infty\\) (since anything else would force \\(\\log(U_n) / \\sqrt n\\) to collapse to 0, which it doesn’t). This means that \\(\\log(U_n)\\) is a random walk that will get arbitrarily high and arbitrarily low, infinitely often, almost surely."
  },
  {
    "objectID": "posts/2023-04-11-uniform-products-solution/index.html#simulations-versus-theory",
    "href": "posts/2023-04-11-uniform-products-solution/index.html#simulations-versus-theory",
    "title": "Uniform Products: Solution",
    "section": "Simulations versus Theory",
    "text": "Simulations versus Theory\nLet’s summarize the end of that story in plain English: when \\(C = e\\), the product \\(U_n\\) will be a random walk in the positive real number line. As the number of digits increases, the product will wander very high up, and it will also wander very close to \\(0\\), and it will do both of those things many times. Here’s a visualization of ten copies of that process, placed on a log scale:\n\n\nShow code\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.random import default_rng\nrng = default_rng(seed=6538)\n\n# length of sample path, i.e. number of uniformly-distributed numbers within\n# each product group\nn = int(1e5)\n\n# generate ten sample paths of random walk, organized in columns\nuniform_nums = rng.uniform(low=0, high=math.e, size=10*n).reshape([n, 10])\n\n# find the cumulative products within columns, starting from the top;\n# the kth element in a given column is the product of the k numbers that lie on \n# or above it\nsample_path = uniform_nums.cumprod(axis = 0)\n\nfig, ax = plt.subplots()\n\n# plot each of the columns as separate paths\nfor i in range(10):\n    ax.plot(np.linspace(1,n,n), sample_path[:, i], alpha=0.5)\n\nax.set_yscale('log')\nax.set_title('Cumulative product vs. number of terms')\nax.set_xlabel('Number of terms in product')\nax.set_ylabel('Product')\n\n# to recreate the graph at the top of the page, uncomment the line below\n# and comment out the ax.set_title() line above\n# ax.set_axis_off()\n\nplt.show()\n\n\n\n\n\nFigure 1: Ten parallel runs of uniform products\n\n\n\n\nThis problem is another great example of how valuable numerical simulations can be – but it’s also a stark reminder of their limitations.\nThanks to simulations, it’s not too hard to see that the funny business occurs somewhat near \\(e\\); however, convincing yourself that the cutoff point is exactly \\(e\\) (and not, say, 2.720 for some reason) can be tricky as a matter of pure numerical tinkering.\nEven if you can convince yourself with simulations that the answer is \\(e\\), it’s quite hard to see the behavior at the critical value of \\(e\\) through simulations alone. If we try to expand one of the products in the graph above to more digits, we run into a problem:\n\n\nShow code\nrng = default_rng(seed=6538)\n\n# length of sample path, i.e. number of uniformly-distributed numbers within\n# each product group\nn = int(1e6)\n\nuniform_nums = rng.uniform(low=0, high=math.e, size=n)\nsample_path = uniform_nums.cumprod()\n\nfig, ax = plt.subplots()\n\nax.plot(np.linspace(1,n,n), sample_path)\n\n# reduce number of ticks on x-axis to avoid\nax.xaxis.set_major_locator(plt.MaxNLocator(3))\n\nax.set_yscale('log')\nax.set_title('Cumulative product vs. number of terms')\nax.set_xlabel('Number of terms in product')\nax.set_ylabel('Product')\n\nplt.show()\n\n\n\n\n\nAt some point, the random walk wandered so close to 0 that Python couldn’t tell it apart from 0 anymore. Restricting ourselves to a mere numerical lens means we miss out on something.\n\nCompared to simulations, mathematical theory is still indispensable when it comes to things like critical values, corner cases, generalizations, and connections between seemingly disparate things. These places are exactly where you can expect to find some of the most important truths as well as some of the most beautiful surprises.\nAnd yet: numerical approaches and simulations do still hint at the wonderful things swimming just beneath the surface, even if they’re not quite enough to look at them directly.\nsome of the math above is reasonably challenging. The reason I like simulations as a pedagogical approach is that they invite someone to explore this phenomenon even if they’re not comfortable with stuff like the math above. Simulations might ask a mathematical explorer to see the beautiful phenomenon; hopefully, they’d then ask why it’s true.\nPROVIDE LINK BACK TO POST"
  }
]