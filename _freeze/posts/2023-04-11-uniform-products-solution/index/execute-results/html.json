{
  "hash": "a289a95b63b70138396df8c83908f44b",
  "result": {
    "markdown": "---\ntitle: \"Uniform Products: Solution\"\nformat:\n  html:\n    fig-width: 9\n    fig-height: 6\n    fig-align: 'center'\ndescription: \"(Read the Problem post first!)\"\nauthor: \"Aaron Montgomery\"\ndate: \"2023-04-11\"\ncategories: [counterintuitive, python-language, simulation]\n---\n\n| ![](random_walks.png) |\n|:--:|\n| <font size = \"2\"> Code to generate this graph can be found above @fig-rw </font>|\n\n# Question\n\nWhen we compute a product of many numbers that are each uniformly distributed\non $[0, C]$, the product becomes very small when $C = 1$ and becomes very \nlarge when $C = 10$. Where is the \"cutoff\" $C$ where the product exhibits a\ntransition?\n\n# Answer\n\nIt's Euler's number, $e \\approx 2.71828...$ -- yes, *that* $e$. It has a habit of\n[doing this sort of thing](https://aaron-montgomery.github.io/amm_blog/posts/2022-07-27-ordered-objects/).\n\nThis deserves an explanation, and we don't have much choice here but to go heavy\non the math here.\n\n## Proof\n\nLet's formulate this mathematically: we want to know how the product\n$$U_n = \\prod_{k=1}^{n} X_k$$\nbehaves, where $\\{X_i\\}$ is a collection of independent, identically-distributed\nnumbers that are all uniformly distributed on $[0, C]$. Products of random \nvariables are somewhat obnoxious to work with; luckily, logarithms[^1] will let \nus trade products for sums.\n$$\\log(U_n) = \\log \\left( \\prod_{k=1}^{n} X_k \\right) = \n\\sum_{k=1}^{n} \\log \\left( X_k \\right)$$\n\nOne advantage of a sum is that it more closely resembles an *average*:\n\n$$\\frac{\\log(U_n)}{n} = \\frac 1 n \\sum_{k=1}^n \\log \\left( X_k \\right)$$\n\nNow, the right-hand side is nothing more than an average of $n$ independent \ncopies of $\\log(X_i)$. The \n[Law of Large Numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers) says \nthat as $n$ increases, the right-hand side will converge to the expected value \n$\\mathbb E[\\log(X_i)]$, so we just need to calculate that. By the \n[Law of the Unconscious Statistician](https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician),\nthis is\n$$\\int_0^C \\frac 1 C \\cdot \\log(x) \\, \\textrm d x = \\frac 1 C \\left[x \\log(x) - x \\right] \\bigg|_0^C = \\log(C) - 1.$$\n\nPutting it all together, we can now investigate the behavior of the original\nproduct $U_n$. As $n$ increases, $\\frac{\\log(U_n)}{n}$ converges to $\\log(C) - 1$.\n\n* When $C > e$, $\\frac{\\log(U_n)}{n}$ converges to a positive constant; hence,\n$\\log(U_n)$ diverges to $\\infty$, so $U_n$ does so as well.\n* When $C < e$, $\\frac{\\log(U_n)}{n}$ converges to a negative constant; in this\ncase, $\\log(U_n)$ diverges to $-\\infty$, implying that $U_n$ converges to $0$.\n\nHence, the \"transition point\" for $C$ is indeed $e$.\n\n## But wait, there's more!\n\nWe left something out! What happens when $C = e$? \n\nLet's back up a bit: since $\\log(U_n) = \\sum \\log(X_k)$, and we have specifically\nchosen a value of $C$ to force $\\mathbb E[\\log(X_k)] = 0$, we can see that\n$\\log(U_n)$ is an unbiased random walk (i.e. a sum whose increments all have \nmean 0). Moreover, these increments are pretty well-behaved. For instance, they \nhave finite second moments:[^2]\n\n$$ \\mathbb E \\left[ \\left(\\log(X_k) \\right)^2 \\right] = \\int_0^e \\frac 1 e \\cdot \\log(x)^2 \\, \\textrm d x = \\frac 1 e \\left[ x(\\log^2(x) - 2 \\log(x) + 2) \\right] \\bigg|_0^e = 1$$\n\nHence, the increments $\\log(X_k)$ have mean $0$ and variance $1$. This \npractically screams for the involvement of everybody's favorite statistical \nworkhorse: the\n[Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem).\nThe CLT shows us that tells us that if we had scaled $\\log(U_n)$ by $\\sqrt n$ \ninstead of by $n$, then $\\frac{\\log(U_n)}{\\sqrt n}$ would do something\ninteresting -- specifically, it would converge (in distribution) to the standard\nnormal distribution. The \n[Hewitt-Savage 0-1 Law](https://en.wikipedia.org/wiki/Hewitt%E2%80%93Savage_zero%E2%80%93one_law)\nimplies that there are some values \n$\\phi_{\\inf}, \\phi_{\\sup} \\in [-\\infty, \\infty]$ such that \n$\\limsup_n \\log(U_n) = \\phi_{\\inf}$ and $\\liminf_n \\log(U_n) = \\phi_{\\sup}$ \nalmost surely; hence, $\\phi_{\\inf} = -\\infty$ and $\\phi_{\\sup} = \\infty$ (since\nanything else would force $\\log(U_n) / \\sqrt n$ to collapse to 0, which it\ndoesn't). This means that $\\log(U_n)$ is a random walk that will get arbitrarily\nhigh and arbitrarily low, infinitely often, almost surely.\n\n# Simulations versus Theory\n\nLet's summarize the end of that story in plain English: when $C = e$, the\nproduct $U_n$ will be a random walk in the positive real number line. As the \nnumber of digits increases, the product will wander very high up, and it will \nalso wander very close to $0$, and it will do both of those things many times. \nHere's a visualization of ten copies of that process, placed on a log scale:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show code\"}\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.random import default_rng\nrng = default_rng(seed=6538)\n\n# length of sample path, i.e. number of uniformly-distributed numbers within\n# each product group\nn = int(1e5)\n\n# generate ten sample paths of random walk, organized in columns\nuniform_nums = rng.uniform(low=0, high=math.e, size=10*n).reshape([n, 10])\n\n# find the cumulative products within columns, starting from the top;\n# the kth element in a given column is the product of the k numbers that lie on \n# or above it\nsample_path = uniform_nums.cumprod(axis = 0)\n\nfig, ax = plt.subplots()\n\n# plot each of the columns as separate paths\nfor i in range(10):\n    ax.plot(np.linspace(1,n,n), sample_path[:, i], alpha=0.5)\n\nax.set_yscale('log')\nax.set_title('Cumulative Products: Random Numbers on [0, e]')\nax.set_xlabel('Number of terms in product')\nax.set_ylabel('Product')\n\n# to recreate the graph at the top of the page, uncomment the line below\n# and comment out the ax.set_title() line above\n# ax.set_axis_off()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Ten parallel runs of uniform products](index_files/figure-html/fig-rw-output-1.png){#fig-rw width=760 height=515}\n:::\n:::\n\n\nThis problem is another great example of how valuable numerical simulations\ncan be -- but it's also a stark reminder of their limitations. \n\nThanks to simulations, it's not too hard to see that the funny business occurs \nsomewhat near $e$; however, convincing yourself that the cutoff point is \n*exactly* $e$ (and not, say, 2.720 for some reason) can be tricky as a matter of \npure numerical tinkering. \n\nEven if you can convince yourself with simulations that the answer is $e$, it's\nquite hard to see what's really going on *at* the critical value $C=e$ through \nsimulations alone. If we try to expand one of the products in the graph above to\nmore digits, we run into a problem:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show code\"}\nrng = default_rng(seed=6538)\n\n# length of sample path, i.e. number of uniformly-distributed numbers within\n# each product group\nn = int(1e6)\n\nuniform_nums = rng.uniform(low=0, high=math.e, size=n)\nsample_path = uniform_nums.cumprod()\n\nfig, ax = plt.subplots()\n\nax.plot(np.linspace(1,n,n), sample_path)\n\n# reduce number of ticks on x-axis to avoid\nax.xaxis.set_major_locator(plt.MaxNLocator(3))\n\nax.set_yscale('log')\nax.set_title('Cumulative Products: Random Numbers on [0, e]')\nax.set_xlabel('Number of terms in product')\nax.set_ylabel('Product')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=760 height=515}\n:::\n:::\n\n\nIn this case, a little bit after 400K digits, the product got so close to 0 that\nPython could not longer distinguish it from 0 at all anymore. This ruins our\nability to see what's *really* happening here; the ideal version of this process\ncannot possibly be *equal* to zero, and if we were patient enough and unbound\nby numerical limitations, we'd see that graph lift back off and head upward \nagain. In this case, restricting ourselves to a mere numerical lens means we \nmiss out on something special.\n\n---\n\nCompared to simulations, mathematical theory is still indispensable when \nit comes to things like critical values, corner cases, generalizations, and \nconnections between seemingly disparate things. These places are exactly where\nyou can expect to find some of the most important truths as well as some of the\nmost beautiful surprises.\n\nAnd yet: numerical approaches and simulations do still hint at the wonderful\nthings swimming just beneath the surface, even if they're not quite enough to\nlook at them directly. As always, using simulations as a sort of\n[mathematical laboratory](https://aaron-montgomery.github.io/amm_blog/posts/2022-07-27-ordered-objects/)\ncan be downright magical, but it's the *beginning* of a story, not its end.\n\n\nPROVIDE LINK BACK TO POST\n\n[^1]: We'll use $\\log(x)$ to denote the natural logarithm, i.e. the inverse of \n$e^{x}$.\n\n[^2]: I swept the details under the rug, but the antiderivative of $\\log^2(x)$\nis genuinely *fun* to find with integration by parts, if you're into that sort\nof thing. Which, of course, I am.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}